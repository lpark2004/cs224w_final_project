{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS224W Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "import torch_geometric \n",
    "import torch\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Union, Tuple, Optional\n",
    "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,\n",
    "                                    OptTensor)\n",
    "\n",
    "from torch.nn import Parameter, Linear\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from ogb.linkproppred import PygLinkPropPredDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rayhotate/miniconda3/envs/cs224w/lib/python3.11/site-packages/ogb/linkproppred/dataset_pyg.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
      "/Users/rayhotate/miniconda3/envs/cs224w/lib/python3.11/site-packages/ogb/linkproppred/dataset_pyg.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'train.pt')))\n",
      "/Users/rayhotate/miniconda3/envs/cs224w/lib/python3.11/site-packages/ogb/linkproppred/dataset_pyg.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  valid = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'valid.pt')))\n",
      "/Users/rayhotate/miniconda3/envs/cs224w/lib/python3.11/site-packages/ogb/linkproppred/dataset_pyg.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'test.pt')))\n"
     ]
    }
   ],
   "source": [
    "dataset = PygLinkPropPredDataset(name = \"ogbl-wikikg2\") \n",
    "\n",
    "split_edge = dataset.get_edge_split()\n",
    "train_edge, valid_edge, test_edge = split_edge[\"train\"], split_edge[\"valid\"], split_edge[\"test\"]\n",
    "graph = dataset[0] # pyg graph object containing only training edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNStack(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, args, emb=False):\n",
    "        super(GNNStack, self).__init__()\n",
    "        conv_model = self.build_conv_model(args.model_type)\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(conv_model(input_dim, hidden_dim, args.num_relations, args.text_dim, args.num_communities))\n",
    "        assert (args.num_layers >= 1), 'Number of layers is not >=1'\n",
    "        for l in range(args.num_layers-1):\n",
    "            self.convs.append(conv_model(args.heads * hidden_dim, hidden_dim, args.num_relations, args.text_dim, args.num_communities))\n",
    "\n",
    "        # post-message-passing\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(args.heads * hidden_dim, hidden_dim), nn.Dropout(args.dropout),\n",
    "            nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.dropout = args.dropout\n",
    "        self.num_layers = args.num_layers\n",
    "\n",
    "        self.emb = emb\n",
    "    \n",
    "    def build_conv_model(self, model_type):\n",
    "        if model_type == \"GAT\":\n",
    "            return GAT  \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_reltype, batch = data.x, data.edge_index, data.edge_reltype, data.batch\n",
    "\n",
    "        x = torch.randn((2, 20))\n",
    "        text_emb = torch.randn(x.shape[0], 100)\n",
    "        community_assign = torch.zeros(x.shape[0], dtype=torch.long)\n",
    "        \n",
    "        # Ensure edge_reltype values are within bounds\n",
    "        edge_reltype = edge_reltype % self.convs[0].num_relations  # Modulo to keep within bounds\n",
    "        edge_reltype = edge_reltype.long()\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index, text_emb, edge_reltype, community_assign)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.post_mp(x)\n",
    "\n",
    "        if self.emb == True:\n",
    "            return x\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancements to the Graph Neural Network Architecture:\n",
    "\n",
    "## 1. Community-based Attention using Leiden Algorithm\n",
    "- Detects densely connected communities in the graph\n",
    "- Allows nodes to attend differently to nodes in same vs different communities  \n",
    "- Leiden algorithm provides high-quality, hierarchical community structure\n",
    "\n",
    "## 2. Hierarchical Attention Mechanism\n",
    "- Local attention: Node-to-node interactions within neighborhoods\n",
    "- Global attention: Node-to-community interactions across graph\n",
    "- Combines both levels for richer graph representations\n",
    "\n",
    "## 3. Text Embedding Integration  \n",
    "- Processes text associated with nodes using embeddings\n",
    "- Projects text features into same space as structural features\n",
    "- Enables multi-modal learning from both graph and text\n",
    "\n",
    "## 4. Relationship-specific Processing\n",
    "- Different weight matrices for different edge types\n",
    "- Allows model to learn relationship-specific transformations\n",
    "- Better handles heterogeneous graph structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(MessagePassing):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels,\n",
    "                 num_relations,\n",
    "                 text_dim,\n",
    "                 num_communities,\n",
    "                 heads=2,\n",
    "                 concat=True,\n",
    "                 negative_slope=0.2,\n",
    "                 dropout=0.,\n",
    "                 add_self_loops=True,\n",
    "                 bias=True,\n",
    "                 **kwargs):\n",
    "        super(GAT, self).__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.num_relations = num_relations\n",
    "        self.text_dim = text_dim\n",
    "        self.num_communities = num_communities\n",
    "        \n",
    "        # Local attention components\n",
    "        self.W_Q = nn.Parameter(torch.Tensor(heads, in_channels, out_channels))\n",
    "        self.W_K = nn.Parameter(torch.Tensor(heads, in_channels, out_channels))\n",
    "        self.W_V = nn.Parameter(torch.Tensor(heads, in_channels, out_channels))\n",
    "        \n",
    "        # Global (community) attention components\n",
    "        self.V_Q = nn.Parameter(torch.Tensor(heads, in_channels, out_channels))\n",
    "        self.V_K = nn.Parameter(torch.Tensor(heads, in_channels, out_channels))\n",
    "        self.V_V = nn.Parameter(torch.Tensor(heads, in_channels, out_channels))\n",
    "        \n",
    "        # Relation-specific components\n",
    "        self.W_r = nn.ParameterList([\n",
    "            nn.Parameter(torch.Tensor(in_channels, out_channels)) \n",
    "            for _ in range(num_relations)\n",
    "        ])\n",
    "        \n",
    "        # Text embedding processing\n",
    "        self.text_proj = nn.Linear(text_dim, out_channels)\n",
    "        self.W_text = nn.ParameterList([\n",
    "            nn.Parameter(torch.Tensor(text_dim, out_channels))\n",
    "            for _ in range(num_relations)\n",
    "        ])\n",
    "        \n",
    "        # Position encodings for communities\n",
    "        self.P_vc = nn.Parameter(torch.Tensor(num_communities, out_channels))\n",
    "        \n",
    "        # Edge-type specific masks\n",
    "        self.M_vu = nn.Parameter(torch.Tensor(num_relations, heads))\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(3 * out_channels, 4 * out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        # Final aggregation MLP\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(out_channels * 4, out_channels * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_channels * 2, out_channels)\n",
    "        )\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        \n",
    "        # Initialize attention components\n",
    "        for param in [self.W_Q, self.W_K, self.W_V, self.V_Q, self.V_K, self.V_V]:\n",
    "            nn.init.xavier_normal_(param, gain=gain)\n",
    "            \n",
    "        # Initialize relation weights\n",
    "        for w_r in self.W_r:\n",
    "            nn.init.xavier_normal_(w_r, gain=gain)\n",
    "            \n",
    "        # Initialize text projections\n",
    "        for w_text in self.W_text:\n",
    "            nn.init.xavier_normal_(w_text, gain=gain)\n",
    "            \n",
    "        # Initialize position encodings and masks\n",
    "        nn.init.xavier_normal_(self.P_vc, gain=gain)\n",
    "        nn.init.xavier_normal_(self.M_vu, gain=gain)\n",
    "\n",
    "    def forward(self, x, edge_index, text_emb, rel_type, community_assign, size=None):\n",
    "        # Local attention\n",
    "        local_out = self._compute_local_attention(x, edge_index, rel_type)\n",
    "        \n",
    "        # Global community attention\n",
    "        global_out = self._compute_global_attention(x, community_assign)\n",
    "        \n",
    "        # Combine local and global attention\n",
    "        combined_struct = local_out + global_out\n",
    "        combined_struct = self.ffn(combined_struct)\n",
    "        \n",
    "        # Process text embeddings\n",
    "        text_out = self._process_text_embeddings(text_emb, edge_index, rel_type)\n",
    "        \n",
    "        # Final aggregation\n",
    "        final_out = self.final_mlp(torch.cat([\n",
    "            combined_struct,\n",
    "            text_out,\n",
    "            x  # Original features as residual\n",
    "        ], dim=-1))\n",
    "        \n",
    "        return final_out\n",
    "\n",
    "    def _compute_local_attention(self, x, edge_index, rel_type):\n",
    "        H = self.heads\n",
    "        \n",
    "        # Compute Q, K, V projections\n",
    "        q = torch.einsum('bhd,ni->bhid', self.W_Q, x)\n",
    "        k = torch.einsum('bhd,ni->bhid', self.W_K, x)\n",
    "        v = torch.einsum('bhd,ni->bhid', self.W_V, x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn_score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.out_channels)\n",
    "        attn_score = attn_score + self.M_vu[rel_type].unsqueeze(-1)\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_weights = F.softmax(attn_score, dim=-1)\n",
    "        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "        \n",
    "        return torch.matmul(attn_weights, v)\n",
    "\n",
    "    def _compute_global_attention(self, x, community_assign):\n",
    "        # Compute community embeddings\n",
    "        community_emb = scatter_mean(x, community_assign, dim=0)\n",
    "        \n",
    "        # Global attention computation\n",
    "        q_global = torch.einsum('bhd,ni->bhid', self.V_Q, x)\n",
    "        k_global = torch.einsum('bhd,ci->bhid', self.V_K, community_emb)\n",
    "        v_global = torch.einsum('bhd,ci->bhid', self.V_V, community_emb)\n",
    "        \n",
    "        # Add position encodings\n",
    "        k_global = k_global + self.P_vc[community_assign]\n",
    "        \n",
    "        # Compute and apply attention\n",
    "        attn_score = torch.matmul(q_global, k_global.transpose(-2, -1)) / math.sqrt(self.out_channels)\n",
    "        attn_weights = F.softmax(attn_score, dim=-1)\n",
    "        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "        \n",
    "        return torch.matmul(attn_weights, v_global)\n",
    "\n",
    "    def _process_text_embeddings(self, text_emb, edge_index, rel_type):\n",
    "        # Project text embeddings\n",
    "        text_proj = self.text_proj(text_emb)\n",
    "        \n",
    "        # Relation-specific text processing\n",
    "        rel_text = torch.einsum('bd,rd->br', text_emb, self.W_text[rel_type])\n",
    "        \n",
    "        return text_proj + rel_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer\n",
    "\n",
    "def train(dataset, args):\n",
    "    test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # build model\n",
    "    model = GNNStack(dataset.num_node_features, args.hidden_dim, dataset.num_classes,\n",
    "                            args)\n",
    "    scheduler, opt = build_optimizer(args, model.parameters())\n",
    "\n",
    "    # train\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            opt.zero_grad()\n",
    "            pred = model(batch)\n",
    "            label = batch.y\n",
    "            pred = pred[batch.train_mask]\n",
    "            label = label[batch.train_mask]\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(loader.dataset)\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "          test_acc = test(test_loader, model)\n",
    "          test_accs.append(test_acc)\n",
    "          if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "          test_accs.append(test_accs[-1])\n",
    "\n",
    "    return test_accs, losses, best_model, best_acc, test_loader\n",
    "\n",
    "\n",
    "def test(loader, test_model, is_validation=False, save_model_preds=False, model_type=None):\n",
    "    test_model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    # Note that Cora is only one graph!\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            # max(dim=1) returns values, indices tuple; only need indices\n",
    "            pred = test_model(data).max(dim=1)[1]\n",
    "            label = data.y\n",
    "\n",
    "        mask = data.val_mask if is_validation else data.test_mask\n",
    "        # node classification: only evaluate on nodes in test set\n",
    "        pred = pred[mask]\n",
    "        label = label[mask]\n",
    "\n",
    "        if save_model_preds:\n",
    "          print (\"Saving Model Predictions for Model Type\", model_type)\n",
    "\n",
    "          data = {}\n",
    "          data['pred'] = pred.view(-1).cpu().detach().numpy()\n",
    "          data['label'] = label.view(-1).cpu().detach().numpy()\n",
    "\n",
    "          df = pd.DataFrame(data=data)\n",
    "          # Save locally as csv\n",
    "          df.to_csv('CORA-Node-' + model_type + '.csv', sep=',', index=False)\n",
    "\n",
    "        correct += pred.eq(label).sum().item()\n",
    "\n",
    "    total = 0\n",
    "    for data in loader.dataset:\n",
    "        total += torch.sum(data.val_mask if is_validation else data.test_mask).item()\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "for args_dict in [\n",
    "    {'model_type': 'GAT', 'dataset': 'cora', 'num_layers': 2, 'heads': 1, 'batch_size': 32, 'hidden_dim': 32, 'dropout': 0.5, 'epochs': 500, 'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, 'weight_decay': 5e-3, 'lr': 0.01, 'num_relations': 3, 'text_dim': 100, 'num_communities': 10},\n",
    "]:\n",
    "    args = SimpleNamespace(**args_dict)\n",
    "    train(dataset, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224w_final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
