{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://snap.stanford.edu/ogb/data/linkproppred/wikikg-v2.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 4.13 GB: 100%|██████████| 4232/4232 [07:49<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/wikikg-v2.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading necessary files...\n",
      "This might take a while.\n",
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 36792.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into PyG objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 313.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n",
      "/Users/rayhotate/miniconda3/envs/cs224w/lib/python3.11/site-packages/ogb/linkproppred/dataset_pyg.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
      "/Users/rayhotate/miniconda3/envs/cs224w/lib/python3.11/site-packages/ogb/linkproppred/dataset_pyg.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'train.pt')))\n",
      "/Users/rayhotate/miniconda3/envs/cs224w/lib/python3.11/site-packages/ogb/linkproppred/dataset_pyg.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  valid = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'valid.pt')))\n",
      "/Users/rayhotate/miniconda3/envs/cs224w/lib/python3.11/site-packages/ogb/linkproppred/dataset_pyg.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'test.pt')))\n"
     ]
    }
   ],
   "source": [
    "from ogb.linkproppred import PygLinkPropPredDataset\n",
    "\n",
    "dataset = PygLinkPropPredDataset(name = \"ogbl-wikikg2\") \n",
    "\n",
    "split_edge = dataset.get_edge_split()\n",
    "train_edge, valid_edge, test_edge = split_edge[\"train\"], split_edge[\"valid\"], split_edge[\"test\"]\n",
    "graph = dataset[0] # pyg graph object containing only training edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric \n",
    "import torch\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Union, Tuple, Optional\n",
    "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,\n",
    "                                    OptTensor)\n",
    "\n",
    "from torch.nn import Parameter, Linear\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNStack(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, args, emb=False):\n",
    "        super(GNNStack, self).__init__()\n",
    "        conv_model = self.build_conv_model(args.model_type)\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(conv_model(input_dim, hidden_dim))\n",
    "        assert (args.num_layers >= 1), 'Number of layers is not >=1'\n",
    "        for l in range(args.num_layers-1):\n",
    "            self.convs.append(conv_model(args.heads * hidden_dim, hidden_dim))\n",
    "\n",
    "        # post-message-passing\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(args.heads * hidden_dim, hidden_dim), nn.Dropout(args.dropout),\n",
    "            nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.dropout = args.dropout\n",
    "        self.num_layers = args.num_layers\n",
    "\n",
    "        self.emb = emb\n",
    "    \n",
    "    def build_conv_model(self, model_type):\n",
    "        if model_type == \"GAT\":\n",
    "            return GAT  \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout,training=self.training)\n",
    "\n",
    "        x = self.post_mp(x)\n",
    "\n",
    "        if self.emb == True:\n",
    "            return x\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancements to the Graph Neural Network Architecture:\n",
    "\n",
    "## 1. Community-based Attention using Leiden Algorithm\n",
    "- Detects densely connected communities in the graph\n",
    "- Allows nodes to attend differently to nodes in same vs different communities  \n",
    "- Leiden algorithm provides high-quality, hierarchical community structure\n",
    "\n",
    "## 2. Hierarchical Attention Mechanism\n",
    "- Local attention: Node-to-node interactions within neighborhoods\n",
    "- Global attention: Node-to-community interactions across graph\n",
    "- Combines both levels for richer graph representations\n",
    "\n",
    "## 3. Text Embedding Integration  \n",
    "- Processes text associated with nodes using embeddings\n",
    "- Projects text features into same space as structural features\n",
    "- Enables multi-modal learning from both graph and text\n",
    "\n",
    "## 4. Relationship-specific Processing\n",
    "- Different weight matrices for different edge types\n",
    "- Allows model to learn relationship-specific transformations\n",
    "- Better handles heterogeneous graph structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(MessagePassing):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels,\n",
    "                 num_relations,\n",
    "                 text_dim,\n",
    "                 num_communities,\n",
    "                 heads=2,\n",
    "                 concat=True,\n",
    "                 negative_slope=0.2,\n",
    "                 dropout=0.,\n",
    "                 add_self_loops=True,\n",
    "                 bias=True,\n",
    "                 **kwargs):\n",
    "        super(GAT, self).__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.num_relations = num_relations\n",
    "        self.text_dim = text_dim\n",
    "        self.num_communities = num_communities\n",
    "        \n",
    "        # Local attention components\n",
    "        self.W_Q = nn.Parameter(torch.Tensor(heads, in_channels, out_channels))\n",
    "        self.W_K = nn.Parameter(torch.Tensor(heads, in_channels, out_channels))\n",
    "        self.W_V = nn.Parameter(torch.Tensor(heads, in_channels, out_channels))\n",
    "        \n",
    "        # Global (community) attention components\n",
    "        self.V_Q = nn.Parameter(torch.Tensor(heads, in_channels, out_channels))\n",
    "        self.V_K = nn.Parameter(torch.Tensor(heads, in_channels, out_channels))\n",
    "        self.V_V = nn.Parameter(torch.Tensor(heads, in_channels, out_channels))\n",
    "        \n",
    "        # Relation-specific components\n",
    "        self.W_r = nn.ParameterList([\n",
    "            nn.Parameter(torch.Tensor(in_channels, out_channels)) \n",
    "            for _ in range(num_relations)\n",
    "        ])\n",
    "        \n",
    "        # Text embedding processing\n",
    "        self.text_proj = nn.Linear(text_dim, out_channels)\n",
    "        self.W_text = nn.ParameterList([\n",
    "            nn.Parameter(torch.Tensor(text_dim, out_channels))\n",
    "            for _ in range(num_relations)\n",
    "        ])\n",
    "        \n",
    "        # Position encodings for communities\n",
    "        self.P_vc = nn.Parameter(torch.Tensor(num_communities, out_channels))\n",
    "        \n",
    "        # Edge-type specific masks\n",
    "        self.M_vu = nn.Parameter(torch.Tensor(num_relations, heads))\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(3 * out_channels, 4 * out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        # Final aggregation MLP\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(out_channels * 4, out_channels * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_channels * 2, out_channels)\n",
    "        )\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        \n",
    "        # Initialize attention components\n",
    "        for param in [self.W_Q, self.W_K, self.W_V, self.V_Q, self.V_K, self.V_V]:\n",
    "            nn.init.xavier_normal_(param, gain=gain)\n",
    "            \n",
    "        # Initialize relation weights\n",
    "        for w_r in self.W_r:\n",
    "            nn.init.xavier_normal_(w_r, gain=gain)\n",
    "            \n",
    "        # Initialize text projections\n",
    "        for w_text in self.W_text:\n",
    "            nn.init.xavier_normal_(w_text, gain=gain)\n",
    "            \n",
    "        # Initialize position encodings and masks\n",
    "        nn.init.xavier_normal_(self.P_vc, gain=gain)\n",
    "        nn.init.xavier_normal_(self.M_vu, gain=gain)\n",
    "\n",
    "    def forward(self, x, edge_index, text_emb, rel_type, community_assign, size=None):\n",
    "        # Local attention\n",
    "        local_out = self._compute_local_attention(x, edge_index, rel_type)\n",
    "        \n",
    "        # Global community attention\n",
    "        global_out = self._compute_global_attention(x, community_assign)\n",
    "        \n",
    "        # Combine local and global attention\n",
    "        combined_struct = local_out + global_out\n",
    "        combined_struct = self.ffn(combined_struct)\n",
    "        \n",
    "        # Process text embeddings\n",
    "        text_out = self._process_text_embeddings(text_emb, edge_index, rel_type)\n",
    "        \n",
    "        # Final aggregation\n",
    "        final_out = self.final_mlp(torch.cat([\n",
    "            combined_struct,\n",
    "            text_out,\n",
    "            x  # Original features as residual\n",
    "        ], dim=-1))\n",
    "        \n",
    "        return final_out\n",
    "\n",
    "    def _compute_local_attention(self, x, edge_index, rel_type):\n",
    "        H = self.heads\n",
    "        \n",
    "        # Compute Q, K, V projections\n",
    "        q = torch.einsum('bhd,ni->bhid', self.W_Q, x)\n",
    "        k = torch.einsum('bhd,ni->bhid', self.W_K, x)\n",
    "        v = torch.einsum('bhd,ni->bhid', self.W_V, x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn_score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.out_channels)\n",
    "        attn_score = attn_score + self.M_vu[rel_type].unsqueeze(-1)\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_weights = F.softmax(attn_score, dim=-1)\n",
    "        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "        \n",
    "        return torch.matmul(attn_weights, v)\n",
    "\n",
    "    def _compute_global_attention(self, x, community_assign):\n",
    "        # Compute community embeddings\n",
    "        community_emb = scatter_mean(x, community_assign, dim=0)\n",
    "        \n",
    "        # Global attention computation\n",
    "        q_global = torch.einsum('bhd,ni->bhid', self.V_Q, x)\n",
    "        k_global = torch.einsum('bhd,ci->bhid', self.V_K, community_emb)\n",
    "        v_global = torch.einsum('bhd,ci->bhid', self.V_V, community_emb)\n",
    "        \n",
    "        # Add position encodings\n",
    "        k_global = k_global + self.P_vc[community_assign]\n",
    "        \n",
    "        # Compute and apply attention\n",
    "        attn_score = torch.matmul(q_global, k_global.transpose(-2, -1)) / math.sqrt(self.out_channels)\n",
    "        attn_weights = F.softmax(attn_score, dim=-1)\n",
    "        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "        \n",
    "        return torch.matmul(attn_weights, v_global)\n",
    "\n",
    "    def _process_text_embeddings(self, text_emb, edge_index, rel_type):\n",
    "        # Project text embeddings\n",
    "        text_proj = self.text_proj(text_emb)\n",
    "        \n",
    "        # Relation-specific text processing\n",
    "        rel_text = torch.einsum('bd,rd->br', text_emb, self.W_text[rel_type])\n",
    "        \n",
    "        return text_proj + rel_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224w_final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
