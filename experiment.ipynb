{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://snap.stanford.edu/ogb/data/linkproppred/wikikg-v2.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 4.13 GB: 100%|██████████| 4232/4232 [07:49<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/wikikg-v2.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading necessary files...\n",
      "This might take a while.\n",
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 36792.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into PyG objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 313.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n",
      "/Users/rayhotate/miniconda3/envs/cs224w/lib/python3.11/site-packages/ogb/linkproppred/dataset_pyg.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
      "/Users/rayhotate/miniconda3/envs/cs224w/lib/python3.11/site-packages/ogb/linkproppred/dataset_pyg.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'train.pt')))\n",
      "/Users/rayhotate/miniconda3/envs/cs224w/lib/python3.11/site-packages/ogb/linkproppred/dataset_pyg.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  valid = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'valid.pt')))\n",
      "/Users/rayhotate/miniconda3/envs/cs224w/lib/python3.11/site-packages/ogb/linkproppred/dataset_pyg.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'test.pt')))\n"
     ]
    }
   ],
   "source": [
    "from ogb.linkproppred import PygLinkPropPredDataset\n",
    "\n",
    "dataset = PygLinkPropPredDataset(name = \"ogbl-wikikg2\") \n",
    "\n",
    "split_edge = dataset.get_edge_split()\n",
    "train_edge, valid_edge, test_edge = split_edge[\"train\"], split_edge[\"valid\"], split_edge[\"test\"]\n",
    "graph = dataset[0] # pyg graph object containing only training edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric \n",
    "import torch\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Union, Tuple, Optional\n",
    "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,\n",
    "                                    OptTensor)\n",
    "\n",
    "from torch.nn import Parameter, Linear\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNStack(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, args, emb=False):\n",
    "        super(GNNStack, self).__init__()\n",
    "        conv_model = self.build_conv_model(args.model_type)\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(conv_model(input_dim, hidden_dim))\n",
    "        assert (args.num_layers >= 1), 'Number of layers is not >=1'\n",
    "        for l in range(args.num_layers-1):\n",
    "            self.convs.append(conv_model(args.heads * hidden_dim, hidden_dim))\n",
    "\n",
    "        # post-message-passing\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(args.heads * hidden_dim, hidden_dim), nn.Dropout(args.dropout),\n",
    "            nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.dropout = args.dropout\n",
    "        self.num_layers = args.num_layers\n",
    "\n",
    "        self.emb = emb\n",
    "    \n",
    "    def build_conv_model(self, model_type):\n",
    "        if model_type == \"GAT\":\n",
    "            return GAT  \n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout,training=self.training)\n",
    "\n",
    "        x = self.post_mp(x)\n",
    "\n",
    "        if self.emb == True:\n",
    "            return x\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's new\n",
    "## New Parameters\n",
    " - concat: Concatenate or average multi-head outputs\n",
    " - add_self_loops: Add self-loops to graph\n",
    " - bias: Use bias in linear layers  \n",
    " - residual: Add residual connections\n",
    " - share_weights: Share weights between source/target transformations\n",
    "\n",
    "## Architectural Improvements \n",
    " - Layer normalization for training stability\n",
    " - Feed-forward network after attention (Transformer-style)\n",
    " - Xavier normal weight initialization\n",
    " - Residual connections with proper dimensionality\n",
    " - GELU activation in feed-forward network\n",
    "\n",
    "## Enhanced Attention\n",
    " - Optional weight sharing between transformations\n",
    " - Proper self-loop handling\n",
    " - Sophisticated aggregation with dimension handling\n",
    "\n",
    "## Structural Improvements\n",
    " - Parameter initialization with gain calculation\n",
    " - Flexible multi-head concatenation\n",
    " - Improved dropout implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(MessagePassing):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 heads=2,\n",
    "                 concat=True,\n",
    "                 negative_slope=0.2, \n",
    "                 dropout=0., \n",
    "                 add_self_loops=True,\n",
    "                 bias=True,\n",
    "                 residual=False,\n",
    "                 share_weights=False,\n",
    "                 **kwargs):\n",
    "        super(GAT, self).__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.residual = residual\n",
    "        self.share_weights = share_weights\n",
    "\n",
    "        # Linear transformations for source and target nodes\n",
    "        self.lin_l = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "        if share_weights:\n",
    "            self.lin_r = self.lin_l\n",
    "        else:\n",
    "            self.lin_r = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        # Attention mechanisms\n",
    "        self.att_l = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_r = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        \n",
    "        # Optional residual connection\n",
    "        if self.residual:\n",
    "            if self.concat:\n",
    "                self.res_fc = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "            else:\n",
    "                self.res_fc = Linear(in_channels, out_channels, bias=bias)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(\n",
    "            heads * out_channels if concat else out_channels\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network after attention\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(heads * out_channels if concat else out_channels,\n",
    "                     4 * (heads * out_channels if concat else out_channels)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * (heads * out_channels if concat else out_channels),\n",
    "                     heads * out_channels if concat else out_channels)\n",
    "        )\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_normal_(self.lin_l.weight, gain=gain)\n",
    "        if not self.share_weights:\n",
    "            nn.init.xavier_normal_(self.lin_r.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.att_l, gain=gain)\n",
    "        nn.init.xavier_normal_(self.att_r, gain=gain)\n",
    "        if self.residual:\n",
    "            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n",
    "        \n",
    "        # Initialize feed-forward layers\n",
    "        for layer in self.feed_forward:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight, gain=gain)\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, size=None):\n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        # Add self-loops to edge_index\n",
    "        if self.add_self_loops:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                num_nodes = x.size(0)\n",
    "                edge_index, _ = remove_self_loops(edge_index)\n",
    "                edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "\n",
    "        # Linear transformations and multi-head splitting\n",
    "        x_l = self.lin_l(x).view(-1, H, C)\n",
    "        x_r = self.lin_r(x).view(-1, H, C)\n",
    "\n",
    "        # Calculate attention coefficients\n",
    "        alpha_l = (x_l * self.att_l).sum(dim=-1)\n",
    "        alpha_r = (x_r * self.att_r).sum(dim=-1)\n",
    "\n",
    "        # Propagate attention-weighted messages\n",
    "        out = self.propagate(edge_index, \n",
    "                           x=(x_r, x_r),\n",
    "                           alpha=(alpha_l, alpha_r), \n",
    "                           size=size)\n",
    "\n",
    "        # Reshape output\n",
    "        if self.concat:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "\n",
    "        # Residual connection\n",
    "        if self.residual:\n",
    "            res = self.res_fc(x)\n",
    "            out = out + res\n",
    "\n",
    "        # Layer normalization\n",
    "        out = self.layer_norm(out)\n",
    "\n",
    "        # Feed-forward network\n",
    "        ff_out = self.feed_forward(out)\n",
    "        \n",
    "        # Final residual connection\n",
    "        out = out + ff_out\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, alpha_j, alpha_i, index, ptr, size_i):\n",
    "        # Attention mechanism\n",
    "        alpha = alpha_i + alpha_j\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        \n",
    "        # Apply feature-wise attention\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        alpha = alpha.unsqueeze(-1)\n",
    "        \n",
    "        return x_j * alpha\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size=None):\n",
    "        # Aggregation with attention weights\n",
    "        return torch_scatter.scatter(inputs, index, dim=0, \n",
    "                                  dim_size=dim_size, reduce='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224w_final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
