{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import *\n",
    "from models import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='ComHG for link prediction')\n",
    "parser.add_argument('--float', default=np.float32)\n",
    "parser.add_argument('--dataset', type=str, default='ogbl-ddi')  # ddi collab ppa citation2\n",
    "parser.add_argument('--result_appendix', type=str, default='', help=\"if '', time as appendix\")\n",
    "parser.add_argument('--device', type=str, default='0', help=\"cpu or gpu id\") \n",
    "parser.add_argument('--directed', type=bool, default=False)\n",
    "parser.add_argument('--coalesce', type=bool, default=True, help=\"whether to coalesce multiple edges between two nodes\")\n",
    "parser.add_argument('--use_weight', type=bool, default=False, help=\"whether to use edge weight.\")\n",
    "parser.add_argument('--use_val', type=bool, default=False, help=\"whether to use edges in validation set for testing\")\n",
    "parser.add_argument('--collab_year', type=int, default=2010, help=\"for ogbl-collab.\")\n",
    "\n",
    "parser.add_argument('--use_feature', type=bool, default=False)\n",
    "parser.add_argument('--use_node_emb', type=bool, default=True)\n",
    "parser.add_argument('--use_dist', type=bool, default=False, help=\"whether to use shortest path distance\")\n",
    "parser.add_argument('--use_cn', type=bool, default=False, help=\"whether to use common neighbor\")\n",
    "parser.add_argument('--use_aa', type=bool, default=False, help=\"whether to use Adamic/Adar\")\n",
    "parser.add_argument('--use_ja', type=bool, default=False, help=\"whether to use Jaccard\")\n",
    "parser.add_argument('--use_ra', type=bool, default=False, help=\"whether to use Resource Allocation\")\n",
    "parser.add_argument('--use_degree', type=bool, default=False)\n",
    "\n",
    "parser.add_argument('--max_dist', type=int, default=5)\n",
    "parser.add_argument('--max_cn', type=int, default=100)\n",
    "parser.add_argument('--max_aa', type=int, default=100)\n",
    "parser.add_argument('--max_ja', type=int, default=100)\n",
    "parser.add_argument('--max_ra', type=int, default=20)\n",
    "parser.add_argument('--max_degree', type=int, default=1000)\n",
    "parser.add_argument('--mag_ja', type=int, default=100)\n",
    "parser.add_argument('--mag_aa', type=int, default=10)\n",
    "parser.add_argument('--mag_ra', type=int, default=10)\n",
    "\n",
    "parser.add_argument('--heurisctic_reproduce', type=bool, default=False, help=\"whether to re-produce heuristics, False would speed up training but may decrease the performance\")\n",
    "parser.add_argument('--heurisctic_batch_size', type=int, default=100, help=\"number of rows in distance computation\")\n",
    "parser.add_argument('--heurisctic_directed', type=bool, default=False, help=\"use directed graph in distance computation\")\n",
    "parser.add_argument('--heurisctic_reuse', type=bool, default=True)\n",
    "parser.add_argument('--neg_size', type=float, default=2000, help=\"size of data_neg_train: neg_size * len(split_edge['train']['edge'])\")\n",
    "\n",
    "parser.add_argument('--adj_hop', type=int, default=2, help=\"hieghest hop of adj for GNN.\")\n",
    "parser.add_argument('--adj_neg', type=float, default=0.0, help=\"neg samples (global neighbors) used in adj. \")\n",
    "parser.add_argument('--adj_neg_dist', type=int, default=10, help=\"distance from global neighbors to the central node.\")\n",
    "parser.add_argument('--adj_weight', type=str, default='same', help=\"the method for weights in the original adj. options: same or decay\")\n",
    "\n",
    "parser.add_argument('--atten_type', type=str, default='Multiply',\n",
    "                    help=\"attention mechnism. Options: Concat, Cosine, Multiply, no_atten, with repsect to GAT, AGNN, Transformer and GCN\")\n",
    "parser.add_argument('--atten_combine', type=str, default='plus',\n",
    "                    help=\"type of combining original adj with attention (if use). must be: plus, multiply or only_atten\")\n",
    "parser.add_argument('--bias', type=bool, default=True)\n",
    "parser.add_argument('--dim_node_emb', type=int, default=512)\n",
    "parser.add_argument('--dim_encoding', type=int, default=32, help=\"dim for encoding heuristics, etc.\")\n",
    "parser.add_argument('--dim_atten', type=int, default=8, help=\"dim for matrix multiplication. Should be small for large graph\")\n",
    "parser.add_argument('--dim_hidden', type=int, default=None, help=\"if None, dim_hidden = dim_in\")\n",
    "parser.add_argument('--n_layers', type=int, default=2, help=\"CNN layers\")\n",
    "parser.add_argument('--n_heads', type=int, default=4, help=\"multiply head CNN\")\n",
    "parser.add_argument('--n_layers_mlp', type=int, default=5)\n",
    "parser.add_argument('--residual', type=bool, default=True, help=\"whether to use residual connection in CNN\")\n",
    "parser.add_argument('--reduce', type=str, default='add', help=\"combine outputs of multi-head CNN modules. options: concat, add\")\n",
    "parser.add_argument('--negative_slope', type=float, default=0.2, help=\"negative_slope for leaky_relu\")\n",
    "\n",
    "parser.add_argument('--num_workers', type=int, default=64)\n",
    "parser.add_argument('--optimizer', type=str, default='Adam', help=\"'Adam', 'AdamW', 'SGD'\")\n",
    "parser.add_argument('--clip_grad_norm', type=float, default=1.0, help=\"whether to use clip_grad_norm_ in training\")\n",
    "parser.add_argument('--use_layer_norm', type=bool, default=False, help=\"whether to use layer norm\")\n",
    "parser.add_argument('--dropout', type=float, default=0.25)\n",
    "parser.add_argument('--dropout_adj', type=float, default=0.)\n",
    "parser.add_argument('--lr', type=float, default=0.002)\n",
    "parser.add_argument('--lr_mini', type=float, default=0.00001, help=\"lr stops decreasing at lr_mini\")\n",
    "parser.add_argument('--scheduler_gamma', type=float, default=0.997)\n",
    "parser.add_argument('--shuffle', type=bool, default=True)\n",
    "\n",
    "parser.add_argument('--runs', type=int, default=1)\n",
    "parser.add_argument('--epochs', type=int, default=1000)\n",
    "parser.add_argument('--eval_epoch', type=int, default=1)\n",
    "parser.add_argument('--batch_size', type=int, default=100000)\n",
    "parser.add_argument('--batch_num', type=int, default=1000, help=\"number of batches trained in an epoch\")\n",
    "\n",
    "parser = add_hierarchical_args(parser)\n",
    "args = parser.parse_args([])  # Empty list since we're in a notebook\n",
    "args.device = device\n",
    "args.max_dist = max(args.max_dist, 3) if args.use_dist else 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved in results/ogbl-ddi_20241212200144\n",
      "Namespace(float=<class 'numpy.float32'>, dataset='ogbl-ddi', result_appendix='_20241212200144', device=device(type='cuda'), directed=False, coalesce=True, use_weight=False, use_val=False, collab_year=2010, use_feature=False, use_node_emb=True, use_dist=False, use_cn=False, use_aa=False, use_ja=False, use_ra=False, use_degree=False, max_dist=3, max_cn=100, max_aa=100, max_ja=100, max_ra=20, max_degree=1000, mag_ja=100, mag_aa=10, mag_ra=10, heurisctic_reproduce=False, heurisctic_batch_size=100, heurisctic_directed=False, heurisctic_reuse=True, neg_size=2000, adj_hop=2, adj_neg=0.0, adj_neg_dist=10, adj_weight='same', atten_type='Multiply', atten_combine='plus', bias=True, dim_node_emb=512, dim_encoding=32, dim_atten=8, dim_hidden=None, n_layers=2, n_heads=4, n_layers_mlp=5, residual=True, reduce='add', negative_slope=0.2, num_workers=64, optimizer='Adam', clip_grad_norm=1.0, use_layer_norm=False, dropout=0.25, dropout_adj=0.0, lr=0.002, lr_mini=1e-05, scheduler_gamma=0.997, shuffle=True, runs=1, epochs=1000, eval_epoch=1, batch_size=100000, batch_num=1000, text_dim=768, relation_dim=100, num_communities=100, eval_metrics='Hits@20', hitK=[20], dense_sparse='dense', dir_result='results/ogbl-ddi_20241212200144')\n"
     ]
    }
   ],
   "source": [
    "# Dataset loading and preprocessing\n",
    "dataset = PygLinkPropPredDataset(name=args.dataset)\n",
    "split_edge = dataset.get_edge_split()\n",
    "data = dataset[0]\n",
    "\n",
    "args.eval_metrics = 'Hits@20'\n",
    "args.hitK = [20]\n",
    "args.dense_sparse = 'dense'\n",
    "\n",
    "# Set up results directory\n",
    "if args.result_appendix == '':\n",
    "    args.result_appendix = '_' + time.strftime(\"%Y%m%d%H%M%S\")\n",
    "args.dir_result = os.path.join('results/{}{}'.format(args.dataset, args.result_appendix))\n",
    "print('Results will be saved in ' + args.dir_result)\n",
    "\n",
    "if not os.path.exists(args.dir_result):\n",
    "    os.makedirs(args.dir_result)\n",
    "\n",
    "# Copy source files to results directory\n",
    "for file in ['main_pred.py', 'utils.py', 'models.py']:\n",
    "    copy(file, args.dir_result)\n",
    "\n",
    "# Set up logging\n",
    "loggers = get_loggers(args)\n",
    "log_file = osp.join(args.dir_result, 'log.log')\n",
    "\n",
    "# Log arguments\n",
    "with open(log_file, 'w') as f:\n",
    "    print(str(args), file=f)\n",
    "    print(str(args), file=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove non-shortest distance neighbors in high adjs ...\n",
      "saving adj_degree data to dataset/ogbl_ddi/processed/adj_hop2_neg0.0.pt\n",
      "finish loading data_pos_train\n",
      "finish loading data_pos_valid\n",
      "finish loading data_neg_valid\n",
      "finish loading data_pos_test\n",
      "finish loading data_neg_test\n"
     ]
    }
   ],
   "source": [
    "data_pos_train = graph_prepare(args, posneg_split='pos_train')\n",
    "data_pos_valid = graph_prepare(args, posneg_split='pos_valid')\n",
    "data_neg_valid = graph_prepare(args, posneg_split='neg_valid')\n",
    "data_pos_test = graph_prepare(args, posneg_split='pos_test')\n",
    "data_neg_test = graph_prepare(args, posneg_split='neg_test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dim_in(args, data):\n",
    "    if data.x != None:\n",
    "        x_size = data.x.size(-1)\n",
    "        if args.dataset == 'ogbl-ppa':\n",
    "            x_size = args.dim_encoding\n",
    "    args.dim_in = 0\n",
    "    if args.use_feature and data.x != None:\n",
    "        args.dim_in += x_size\n",
    "    if args.use_node_emb:\n",
    "        args.dim_in += args.dim_node_emb\n",
    "    if args.use_degree:\n",
    "        args.dim_in += args.dim_encoding\n",
    "    if args.dim_in == 0:\n",
    "        args.dim_in = 1\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_DataLoader(args, data, shuffle=False):\n",
    "    edges = data.edges\n",
    "    num_edge = edges.size(0)\n",
    "    perm_size = int(min(args.batch_num * args.batch_size, num_edge))\n",
    "    if shuffle:\n",
    "        if num_edge > 1E8:\n",
    "            perm = np.array(random.sample(range(num_edge), perm_size))\n",
    "        else:\n",
    "            perm = np.random.permutation(num_edge)\n",
    "            perm = perm[:perm_size]\n",
    "        edges = edges[perm]\n",
    "    step, end = args.batch_size, perm_size\n",
    "    perms = [np.array(range(i, i + step)) if i + step < end else np.array(range(i, end)) for i in range(0, end, step)]\n",
    "\n",
    "    return edges, perms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, predictor, optimizer, scheduler, data_pos, data_neg):\n",
    "    #\n",
    "    predictor.train()\n",
    "    running_loss = running_examples = 0\n",
    "\n",
    "    pos_edges, pos_perms = my_DataLoader(args, data_pos, shuffle=args.shuffle)\n",
    "    neg_edges, neg_perms = my_DataLoader(args, data_neg, shuffle=args.shuffle)\n",
    "    leniter = min(len(pos_perms), len(neg_perms))\n",
    "    for i in range(leniter):\n",
    "        optimizer.zero_grad()\n",
    "        edge_pos = pos_edges[pos_perms[i]]\n",
    "        edge_neg = neg_edges[neg_perms[i]]\n",
    "        y_pos = predictor(data_pos, edge_pos)\n",
    "        y_neg = predictor(data_neg, edge_neg)\n",
    "        # avoid imbalance\n",
    "        if len(y_pos) != len(y_neg):\n",
    "            break\n",
    "\n",
    "        pos_loss = (-torch.log(y_pos + 1e-15)).mean()\n",
    "        neg_loss = (-torch.log(1.0 - y_neg + 1e-15)).mean()\n",
    "        loss = pos_loss + neg_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(predictor.parameters(), args.clip_grad_norm)#, error_if_nonfinite=True\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_examples += 1\n",
    "    return running_loss / running_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(args, predictor, data_pos, data_neg):\n",
    "    predictor.eval()\n",
    "\n",
    "    def get_predictions(args, predictor, data_pred):\n",
    "        pred = []\n",
    "        edges = data_pred.edges\n",
    "        num_edge = edges.size(0)\n",
    "        step, end = args.batch_size*100, num_edge\n",
    "        perms = [np.array(range(i, i + step)) if i + step < end else np.array(range(i, end)) for i in range(0, end, step)]\n",
    "        for perm in perms:\n",
    "            edge_batch = edges[perm]\n",
    "            y = predictor(data_pred, edge_batch)\n",
    "            pred.append(y.detach().cpu().squeeze(1))\n",
    "        return torch.cat(pred, dim=0)\n",
    "\n",
    "    pred = torch.cat([get_predictions(args, predictor,data_pos), get_predictions(args, predictor,data_neg)], dim=0)\n",
    "    true = torch.cat([torch.ones(data_pos.edges.size(0)), torch.zeros(data_neg.edges.size(0))], dim=0)\n",
    "\n",
    "    return pred, true\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative sampling ...\n",
      "negative sampling finished\n",
      "finish loading data_neg_train\n",
      "count_parameters: 8896994\n"
     ]
    }
   ],
   "source": [
    "data_neg_train = graph_prepare(args, posneg_split='neg_train')\n",
    "\n",
    "# tensorboard_writer = SummaryWriter(osp.join(args.dir_result, f'log_{run}.log'))\n",
    "args = get_dim_in(args, data_pos_train)\n",
    "predictor = HierarchicalComHG(args).to(args.device)\n",
    "print('count_parameters:', count_parameters(predictor))\n",
    "optimizer = get_optimizer(args, predictor.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=args.scheduler_gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 0.0020000\n",
      ": Hits@20: Epoch: 00, Loss: 1.4064, Valid: 0.00%, Test: 0.00%\n",
      "learning rate: 0.0019940\n",
      ": Hits@20: Epoch: 01, Loss: 1.3706, Valid: 0.79%, Test: 0.71%\n",
      "learning rate: 0.0019880\n",
      ": Hits@20: Epoch: 02, Loss: 0.9284, Valid: 2.00%, Test: 2.96%\n",
      "learning rate: 0.0019821\n",
      ": Hits@20: Epoch: 03, Loss: 0.8855, Valid: 1.76%, Test: 1.70%\n",
      "learning rate: 0.0019761\n",
      ": Hits@20: Epoch: 04, Loss: 0.8662, Valid: 1.42%, Test: 1.24%\n",
      "learning rate: 0.0019702\n",
      ": Hits@20: Epoch: 05, Loss: 0.8338, Valid: 3.98%, Test: 2.53%\n",
      "learning rate: 0.0019643\n",
      ": Hits@20: Epoch: 06, Loss: 0.7766, Valid: 4.78%, Test: 3.15%\n",
      "learning rate: 0.0019584\n",
      ": Hits@20: Epoch: 07, Loss: 0.6996, Valid: 4.86%, Test: 6.97%\n",
      "learning rate: 0.0019525\n",
      ": Hits@20: Epoch: 08, Loss: 0.6500, Valid: 8.67%, Test: 3.83%\n",
      "learning rate: 0.0019466\n",
      ": Hits@20: Epoch: 09, Loss: 0.6269, Valid: 9.95%, Test: 5.85%\n",
      "learning rate: 0.0019408\n",
      ": Hits@20: Epoch: 10, Loss: 0.6076, Valid: 9.90%, Test: 2.34%\n",
      "learning rate: 0.0019350\n",
      ": Hits@20: Epoch: 11, Loss: 0.6089, Valid: 7.30%, Test: 4.18%\n",
      "learning rate: 0.0019292\n",
      ": Hits@20: Epoch: 12, Loss: 0.6145, Valid: 19.76%, Test: 13.50%\n",
      "learning rate: 0.0019234\n",
      ": Hits@20: Epoch: 13, Loss: 0.5728, Valid: 18.08%, Test: 13.27%\n",
      "learning rate: 0.0019176\n",
      ": Hits@20: Epoch: 14, Loss: 0.5421, Valid: 23.95%, Test: 16.07%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0019119\n",
      ": Hits@20: Epoch: 15, Loss: 0.5410, Valid: 22.48%, Test: 11.88%\n",
      "learning rate: 0.0019061\n",
      ": Hits@20: Epoch: 16, Loss: 0.5268, Valid: 18.63%, Test: 8.73%\n",
      "learning rate: 0.0019004\n",
      ": Hits@20: Epoch: 17, Loss: 0.4984, Valid: 19.10%, Test: 9.19%\n",
      "learning rate: 0.0018947\n",
      ": Hits@20: Epoch: 18, Loss: 0.4825, Valid: 20.75%, Test: 7.19%\n",
      "learning rate: 0.0018890\n",
      ": Hits@20: Epoch: 19, Loss: 0.4674, Valid: 12.20%, Test: 3.74%\n",
      "learning rate: 0.0018834\n",
      ": Hits@20: Epoch: 20, Loss: 0.4676, Valid: 26.50%, Test: 14.42%\n",
      "learning rate: 0.0018777\n",
      ": Hits@20: Epoch: 21, Loss: 0.4862, Valid: 13.62%, Test: 3.05%\n",
      "learning rate: 0.0018721\n",
      ": Hits@20: Epoch: 22, Loss: 0.4703, Valid: 14.88%, Test: 6.98%\n",
      "learning rate: 0.0018665\n",
      ": Hits@20: Epoch: 23, Loss: 0.4354, Valid: 15.44%, Test: 4.56%\n",
      "learning rate: 0.0018609\n",
      ": Hits@20: Epoch: 24, Loss: 0.4236, Valid: 12.93%, Test: 2.78%\n",
      "learning rate: 0.0018553\n",
      ": Hits@20: Epoch: 25, Loss: 0.4079, Valid: 12.78%, Test: 5.19%\n",
      "learning rate: 0.0018497\n",
      ": Hits@20: Epoch: 26, Loss: 0.3928, Valid: 17.22%, Test: 4.91%\n",
      "learning rate: 0.0018442\n",
      ": Hits@20: Epoch: 27, Loss: 0.3796, Valid: 17.67%, Test: 4.58%\n",
      "learning rate: 0.0018386\n",
      ": Hits@20: Epoch: 28, Loss: 0.3739, Valid: 13.82%, Test: 3.83%\n",
      "learning rate: 0.0018331\n",
      ": Hits@20: Epoch: 29, Loss: 0.3685, Valid: 16.93%, Test: 3.52%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0018276\n",
      ": Hits@20: Epoch: 30, Loss: 0.3543, Valid: 19.68%, Test: 5.70%\n",
      "learning rate: 0.0018221\n",
      ": Hits@20: Epoch: 31, Loss: 0.3428, Valid: 15.13%, Test: 4.82%\n",
      "learning rate: 0.0018167\n",
      ": Hits@20: Epoch: 32, Loss: 0.3426, Valid: 15.52%, Test: 4.79%\n",
      "learning rate: 0.0018112\n",
      ": Hits@20: Epoch: 33, Loss: 0.3329, Valid: 15.62%, Test: 3.58%\n",
      "learning rate: 0.0018058\n",
      ": Hits@20: Epoch: 34, Loss: 0.3252, Valid: 23.11%, Test: 5.30%\n",
      "learning rate: 0.0018004\n",
      ": Hits@20: Epoch: 35, Loss: 0.3201, Valid: 22.41%, Test: 3.94%\n",
      "learning rate: 0.0017950\n",
      ": Hits@20: Epoch: 36, Loss: 0.3175, Valid: 19.65%, Test: 4.50%\n",
      "learning rate: 0.0017896\n",
      ": Hits@20: Epoch: 37, Loss: 0.3106, Valid: 21.07%, Test: 6.68%\n",
      "learning rate: 0.0017842\n",
      ": Hits@20: Epoch: 38, Loss: 0.3090, Valid: 19.36%, Test: 4.58%\n",
      "learning rate: 0.0017789\n",
      ": Hits@20: Epoch: 39, Loss: 0.3067, Valid: 22.94%, Test: 10.05%\n",
      "learning rate: 0.0017735\n",
      ": Hits@20: Epoch: 40, Loss: 0.2991, Valid: 19.54%, Test: 5.31%\n",
      "learning rate: 0.0017682\n",
      ": Hits@20: Epoch: 41, Loss: 0.2933, Valid: 24.59%, Test: 9.27%\n",
      "learning rate: 0.0017629\n",
      ": Hits@20: Epoch: 42, Loss: 0.2892, Valid: 28.06%, Test: 8.43%\n",
      "learning rate: 0.0017576\n",
      ": Hits@20: Epoch: 43, Loss: 0.2876, Valid: 17.79%, Test: 4.63%\n",
      "learning rate: 0.0017523\n",
      ": Hits@20: Epoch: 44, Loss: 0.2869, Valid: 26.24%, Test: 6.64%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0017471\n",
      ": Hits@20: Epoch: 45, Loss: 0.2804, Valid: 30.76%, Test: 8.61%\n",
      "learning rate: 0.0017418\n",
      ": Hits@20: Epoch: 46, Loss: 0.2792, Valid: 20.06%, Test: 5.42%\n",
      "learning rate: 0.0017366\n",
      ": Hits@20: Epoch: 47, Loss: 0.2779, Valid: 24.47%, Test: 8.23%\n",
      "learning rate: 0.0017314\n",
      ": Hits@20: Epoch: 48, Loss: 0.2777, Valid: 28.35%, Test: 7.23%\n",
      "learning rate: 0.0017262\n",
      ": Hits@20: Epoch: 49, Loss: 0.2766, Valid: 30.71%, Test: 6.30%\n",
      "learning rate: 0.0017210\n",
      ": Hits@20: Epoch: 50, Loss: 0.2694, Valid: 30.11%, Test: 7.76%\n",
      "learning rate: 0.0017159\n",
      ": Hits@20: Epoch: 51, Loss: 0.2654, Valid: 27.30%, Test: 5.83%\n",
      "learning rate: 0.0017107\n",
      ": Hits@20: Epoch: 52, Loss: 0.2636, Valid: 30.68%, Test: 10.12%\n",
      "learning rate: 0.0017056\n",
      ": Hits@20: Epoch: 53, Loss: 0.2583, Valid: 30.12%, Test: 11.19%\n",
      "learning rate: 0.0017005\n",
      ": Hits@20: Epoch: 54, Loss: 0.2578, Valid: 32.51%, Test: 8.50%\n",
      "learning rate: 0.0016954\n",
      ": Hits@20: Epoch: 55, Loss: 0.2548, Valid: 27.47%, Test: 13.02%\n",
      "learning rate: 0.0016903\n",
      ": Hits@20: Epoch: 56, Loss: 0.2535, Valid: 26.81%, Test: 9.84%\n",
      "learning rate: 0.0016852\n",
      ": Hits@20: Epoch: 57, Loss: 0.2494, Valid: 30.70%, Test: 12.70%\n",
      "learning rate: 0.0016802\n",
      ": Hits@20: Epoch: 58, Loss: 0.2493, Valid: 34.93%, Test: 14.29%\n",
      "learning rate: 0.0016751\n",
      ": Hits@20: Epoch: 59, Loss: 0.2489, Valid: 28.87%, Test: 10.52%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0016701\n",
      ": Hits@20: Epoch: 60, Loss: 0.2448, Valid: 30.98%, Test: 14.58%\n",
      "learning rate: 0.0016651\n",
      ": Hits@20: Epoch: 61, Loss: 0.2416, Valid: 38.19%, Test: 22.24%\n",
      "learning rate: 0.0016601\n",
      ": Hits@20: Epoch: 62, Loss: 0.2397, Valid: 35.74%, Test: 20.27%\n",
      "learning rate: 0.0016551\n",
      ": Hits@20: Epoch: 63, Loss: 0.2379, Valid: 33.36%, Test: 19.93%\n",
      "learning rate: 0.0016501\n",
      ": Hits@20: Epoch: 64, Loss: 0.2376, Valid: 30.26%, Test: 20.32%\n",
      "learning rate: 0.0016452\n",
      ": Hits@20: Epoch: 65, Loss: 0.2378, Valid: 39.09%, Test: 20.52%\n",
      "learning rate: 0.0016403\n",
      ": Hits@20: Epoch: 66, Loss: 0.2363, Valid: 33.43%, Test: 16.11%\n",
      "learning rate: 0.0016353\n",
      ": Hits@20: Epoch: 67, Loss: 0.2364, Valid: 39.36%, Test: 18.17%\n",
      "learning rate: 0.0016304\n",
      ": Hits@20: Epoch: 68, Loss: 0.2329, Valid: 36.84%, Test: 17.57%\n",
      "learning rate: 0.0016255\n",
      ": Hits@20: Epoch: 69, Loss: 0.2293, Valid: 32.98%, Test: 17.31%\n",
      "learning rate: 0.0016207\n",
      ": Hits@20: Epoch: 70, Loss: 0.2262, Valid: 38.89%, Test: 18.74%\n",
      "learning rate: 0.0016158\n",
      ": Hits@20: Epoch: 71, Loss: 0.2266, Valid: 43.35%, Test: 17.79%\n",
      "learning rate: 0.0016109\n",
      ": Hits@20: Epoch: 72, Loss: 0.2253, Valid: 37.05%, Test: 19.29%\n",
      "learning rate: 0.0016061\n",
      ": Hits@20: Epoch: 73, Loss: 0.2238, Valid: 38.91%, Test: 24.83%\n",
      "learning rate: 0.0016013\n",
      ": Hits@20: Epoch: 74, Loss: 0.2276, Valid: 32.28%, Test: 18.37%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0015965\n",
      ": Hits@20: Epoch: 75, Loss: 0.2234, Valid: 43.17%, Test: 16.68%\n",
      "learning rate: 0.0015917\n",
      ": Hits@20: Epoch: 76, Loss: 0.2194, Valid: 44.55%, Test: 18.09%\n",
      "learning rate: 0.0015869\n",
      ": Hits@20: Epoch: 77, Loss: 0.2172, Valid: 45.24%, Test: 20.71%\n",
      "learning rate: 0.0015822\n",
      ": Hits@20: Epoch: 78, Loss: 0.2162, Valid: 40.02%, Test: 15.34%\n",
      "learning rate: 0.0015774\n",
      ": Hits@20: Epoch: 79, Loss: 0.2147, Valid: 43.25%, Test: 22.82%\n",
      "learning rate: 0.0015727\n",
      ": Hits@20: Epoch: 80, Loss: 0.2140, Valid: 44.25%, Test: 18.22%\n",
      "learning rate: 0.0015680\n",
      ": Hits@20: Epoch: 81, Loss: 0.2132, Valid: 42.35%, Test: 17.36%\n",
      "learning rate: 0.0015633\n",
      ": Hits@20: Epoch: 82, Loss: 0.2112, Valid: 47.21%, Test: 19.94%\n",
      "learning rate: 0.0015586\n",
      ": Hits@20: Epoch: 83, Loss: 0.2103, Valid: 42.21%, Test: 22.44%\n",
      "learning rate: 0.0015539\n",
      ": Hits@20: Epoch: 84, Loss: 0.2095, Valid: 50.13%, Test: 18.14%\n",
      "learning rate: 0.0015492\n",
      ": Hits@20: Epoch: 85, Loss: 0.2072, Valid: 45.61%, Test: 18.56%\n",
      "learning rate: 0.0015446\n",
      ": Hits@20: Epoch: 86, Loss: 0.2077, Valid: 47.17%, Test: 20.81%\n",
      "learning rate: 0.0015400\n",
      ": Hits@20: Epoch: 87, Loss: 0.2057, Valid: 41.86%, Test: 16.30%\n",
      "learning rate: 0.0015353\n",
      ": Hits@20: Epoch: 88, Loss: 0.2081, Valid: 48.40%, Test: 17.58%\n",
      "learning rate: 0.0015307\n",
      ": Hits@20: Epoch: 89, Loss: 0.2087, Valid: 46.57%, Test: 20.63%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0015261\n",
      ": Hits@20: Epoch: 90, Loss: 0.2043, Valid: 47.47%, Test: 16.66%\n",
      "learning rate: 0.0015216\n",
      ": Hits@20: Epoch: 91, Loss: 0.1996, Valid: 47.71%, Test: 22.21%\n",
      "learning rate: 0.0015170\n",
      ": Hits@20: Epoch: 92, Loss: 0.1991, Valid: 43.41%, Test: 15.91%\n",
      "learning rate: 0.0015124\n",
      ": Hits@20: Epoch: 93, Loss: 0.1982, Valid: 48.35%, Test: 13.97%\n",
      "learning rate: 0.0015079\n",
      ": Hits@20: Epoch: 94, Loss: 0.1990, Valid: 51.22%, Test: 18.83%\n",
      "learning rate: 0.0015034\n",
      ": Hits@20: Epoch: 95, Loss: 0.1967, Valid: 51.69%, Test: 17.84%\n",
      "learning rate: 0.0014989\n",
      ": Hits@20: Epoch: 96, Loss: 0.1960, Valid: 50.29%, Test: 18.04%\n",
      "learning rate: 0.0014944\n",
      ": Hits@20: Epoch: 97, Loss: 0.1961, Valid: 51.55%, Test: 18.48%\n",
      "learning rate: 0.0014899\n",
      ": Hits@20: Epoch: 98, Loss: 0.1931, Valid: 51.53%, Test: 17.89%\n",
      "learning rate: 0.0014854\n",
      ": Hits@20: Epoch: 99, Loss: 0.1923, Valid: 52.88%, Test: 14.11%\n",
      "learning rate: 0.0014810\n",
      ": Hits@20: Epoch: 100, Loss: 0.1919, Valid: 50.53%, Test: 10.39%\n",
      "learning rate: 0.0014765\n",
      ": Hits@20: Epoch: 101, Loss: 0.1970, Valid: 50.42%, Test: 15.43%\n",
      "learning rate: 0.0014721\n",
      ": Hits@20: Epoch: 102, Loss: 0.1954, Valid: 44.31%, Test: 9.81%\n",
      "learning rate: 0.0014677\n",
      ": Hits@20: Epoch: 103, Loss: 0.1917, Valid: 48.48%, Test: 14.33%\n",
      "learning rate: 0.0014633\n",
      ": Hits@20: Epoch: 104, Loss: 0.1897, Valid: 50.44%, Test: 15.47%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0014589\n",
      ": Hits@20: Epoch: 105, Loss: 0.1872, Valid: 49.96%, Test: 9.87%\n",
      "learning rate: 0.0014545\n",
      ": Hits@20: Epoch: 106, Loss: 0.1879, Valid: 52.10%, Test: 9.83%\n",
      "learning rate: 0.0014501\n",
      ": Hits@20: Epoch: 107, Loss: 0.1877, Valid: 52.08%, Test: 10.51%\n",
      "learning rate: 0.0014458\n",
      ": Hits@20: Epoch: 108, Loss: 0.1865, Valid: 51.81%, Test: 8.02%\n",
      "learning rate: 0.0014415\n",
      ": Hits@20: Epoch: 109, Loss: 0.1866, Valid: 51.36%, Test: 7.40%\n",
      "learning rate: 0.0014371\n",
      ": Hits@20: Epoch: 110, Loss: 0.1852, Valid: 51.80%, Test: 8.27%\n",
      "learning rate: 0.0014328\n",
      ": Hits@20: Epoch: 111, Loss: 0.1830, Valid: 55.18%, Test: 7.78%\n",
      "learning rate: 0.0014285\n",
      ": Hits@20: Epoch: 112, Loss: 0.1829, Valid: 53.35%, Test: 11.16%\n",
      "learning rate: 0.0014242\n",
      ": Hits@20: Epoch: 113, Loss: 0.1822, Valid: 57.63%, Test: 12.10%\n",
      "learning rate: 0.0014200\n",
      ": Hits@20: Epoch: 114, Loss: 0.1829, Valid: 53.68%, Test: 8.05%\n",
      "learning rate: 0.0014157\n",
      ": Hits@20: Epoch: 115, Loss: 0.1816, Valid: 58.03%, Test: 11.57%\n",
      "learning rate: 0.0014115\n",
      ": Hits@20: Epoch: 116, Loss: 0.1816, Valid: 56.31%, Test: 8.43%\n",
      "learning rate: 0.0014072\n",
      ": Hits@20: Epoch: 117, Loss: 0.1805, Valid: 58.95%, Test: 7.72%\n",
      "learning rate: 0.0014030\n",
      ": Hits@20: Epoch: 118, Loss: 0.1794, Valid: 57.98%, Test: 9.08%\n",
      "learning rate: 0.0013988\n",
      ": Hits@20: Epoch: 119, Loss: 0.1795, Valid: 62.72%, Test: 7.03%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0013946\n",
      ": Hits@20: Epoch: 120, Loss: 0.1783, Valid: 57.82%, Test: 7.21%\n",
      "learning rate: 0.0013904\n",
      ": Hits@20: Epoch: 121, Loss: 0.1779, Valid: 59.25%, Test: 8.68%\n",
      "learning rate: 0.0013862\n",
      ": Hits@20: Epoch: 122, Loss: 0.1787, Valid: 61.84%, Test: 6.54%\n",
      "learning rate: 0.0013821\n",
      ": Hits@20: Epoch: 123, Loss: 0.1786, Valid: 62.81%, Test: 9.26%\n",
      "learning rate: 0.0013779\n",
      ": Hits@20: Epoch: 124, Loss: 0.1767, Valid: 61.89%, Test: 6.54%\n",
      "learning rate: 0.0013738\n",
      ": Hits@20: Epoch: 125, Loss: 0.1765, Valid: 62.12%, Test: 8.66%\n",
      "learning rate: 0.0013697\n",
      ": Hits@20: Epoch: 126, Loss: 0.1747, Valid: 62.17%, Test: 6.15%\n",
      "learning rate: 0.0013656\n",
      ": Hits@20: Epoch: 127, Loss: 0.1764, Valid: 59.60%, Test: 4.65%\n",
      "learning rate: 0.0013615\n",
      ": Hits@20: Epoch: 128, Loss: 0.1753, Valid: 58.08%, Test: 15.23%\n",
      "learning rate: 0.0013574\n",
      ": Hits@20: Epoch: 129, Loss: 0.1736, Valid: 64.35%, Test: 6.19%\n",
      "learning rate: 0.0013533\n",
      ": Hits@20: Epoch: 130, Loss: 0.1728, Valid: 63.91%, Test: 6.42%\n",
      "learning rate: 0.0013493\n",
      ": Hits@20: Epoch: 131, Loss: 0.1740, Valid: 62.89%, Test: 11.71%\n",
      "learning rate: 0.0013452\n",
      ": Hits@20: Epoch: 132, Loss: 0.1722, Valid: 62.68%, Test: 5.37%\n",
      "learning rate: 0.0013412\n",
      ": Hits@20: Epoch: 133, Loss: 0.1722, Valid: 63.58%, Test: 7.87%\n",
      "learning rate: 0.0013372\n",
      ": Hits@20: Epoch: 134, Loss: 0.1718, Valid: 58.61%, Test: 7.22%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0013331\n",
      ": Hits@20: Epoch: 135, Loss: 0.1708, Valid: 63.38%, Test: 9.23%\n",
      "learning rate: 0.0013291\n",
      ": Hits@20: Epoch: 136, Loss: 0.1705, Valid: 60.39%, Test: 7.95%\n",
      "learning rate: 0.0013252\n",
      ": Hits@20: Epoch: 137, Loss: 0.1707, Valid: 65.07%, Test: 7.82%\n",
      "learning rate: 0.0013212\n",
      ": Hits@20: Epoch: 138, Loss: 0.1701, Valid: 64.73%, Test: 11.58%\n",
      "learning rate: 0.0013172\n",
      ": Hits@20: Epoch: 139, Loss: 0.1685, Valid: 65.75%, Test: 29.24%\n",
      "learning rate: 0.0013133\n",
      ": Hits@20: Epoch: 140, Loss: 0.1693, Valid: 63.57%, Test: 6.82%\n",
      "learning rate: 0.0013093\n",
      ": Hits@20: Epoch: 141, Loss: 0.1692, Valid: 60.95%, Test: 7.52%\n",
      "learning rate: 0.0013054\n",
      ": Hits@20: Epoch: 142, Loss: 0.1689, Valid: 65.61%, Test: 23.23%\n",
      "learning rate: 0.0013015\n",
      ": Hits@20: Epoch: 143, Loss: 0.1688, Valid: 64.85%, Test: 5.24%\n",
      "learning rate: 0.0012976\n",
      ": Hits@20: Epoch: 144, Loss: 0.1676, Valid: 63.28%, Test: 4.48%\n",
      "learning rate: 0.0012937\n",
      ": Hits@20: Epoch: 145, Loss: 0.1675, Valid: 62.29%, Test: 8.84%\n",
      "learning rate: 0.0012898\n",
      ": Hits@20: Epoch: 146, Loss: 0.1675, Valid: 62.22%, Test: 6.13%\n",
      "learning rate: 0.0012859\n",
      ": Hits@20: Epoch: 147, Loss: 0.1669, Valid: 65.60%, Test: 12.99%\n",
      "learning rate: 0.0012821\n",
      ": Hits@20: Epoch: 148, Loss: 0.1673, Valid: 65.55%, Test: 7.16%\n",
      "learning rate: 0.0012782\n",
      ": Hits@20: Epoch: 149, Loss: 0.1695, Valid: 64.97%, Test: 6.57%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0012744\n",
      ": Hits@20: Epoch: 150, Loss: 0.3075, Valid: 33.48%, Test: 22.72%\n",
      "learning rate: 0.0012706\n",
      ": Hits@20: Epoch: 151, Loss: 0.3394, Valid: 43.49%, Test: 30.13%\n",
      "learning rate: 0.0012668\n",
      ": Hits@20: Epoch: 152, Loss: 0.2692, Valid: 25.14%, Test: 43.97%\n",
      "learning rate: 0.0012630\n",
      ": Hits@20: Epoch: 153, Loss: 0.2125, Valid: 51.64%, Test: 32.16%\n",
      "learning rate: 0.0012592\n",
      ": Hits@20: Epoch: 154, Loss: 0.1895, Valid: 53.17%, Test: 15.90%\n",
      "learning rate: 0.0012554\n",
      ": Hits@20: Epoch: 155, Loss: 0.1794, Valid: 58.28%, Test: 11.74%\n",
      "learning rate: 0.0012516\n",
      ": Hits@20: Epoch: 156, Loss: 0.1736, Valid: 58.79%, Test: 8.10%\n",
      "learning rate: 0.0012479\n",
      ": Hits@20: Epoch: 157, Loss: 0.1704, Valid: 60.03%, Test: 11.76%\n",
      "learning rate: 0.0012441\n",
      ": Hits@20: Epoch: 158, Loss: 0.1700, Valid: 60.61%, Test: 9.09%\n",
      "learning rate: 0.0012404\n",
      ": Hits@20: Epoch: 159, Loss: 0.1685, Valid: 59.30%, Test: 19.69%\n",
      "learning rate: 0.0012367\n",
      ": Hits@20: Epoch: 160, Loss: 0.1671, Valid: 64.16%, Test: 23.84%\n",
      "learning rate: 0.0012330\n",
      ": Hits@20: Epoch: 161, Loss: 0.1661, Valid: 65.45%, Test: 24.18%\n",
      "learning rate: 0.0012293\n",
      ": Hits@20: Epoch: 162, Loss: 0.1655, Valid: 65.22%, Test: 10.79%\n",
      "learning rate: 0.0012256\n",
      ": Hits@20: Epoch: 163, Loss: 0.1644, Valid: 62.95%, Test: 22.91%\n",
      "learning rate: 0.0012219\n",
      ": Hits@20: Epoch: 164, Loss: 0.1652, Valid: 64.54%, Test: 18.49%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0012182\n",
      ": Hits@20: Epoch: 165, Loss: 0.1620, Valid: 65.31%, Test: 20.05%\n",
      "learning rate: 0.0012146\n",
      ": Hits@20: Epoch: 166, Loss: 0.1626, Valid: 65.50%, Test: 24.85%\n",
      "learning rate: 0.0012109\n",
      ": Hits@20: Epoch: 167, Loss: 0.1622, Valid: 66.14%, Test: 46.66%\n",
      "learning rate: 0.0012073\n",
      ": Hits@20: Epoch: 168, Loss: 0.1621, Valid: 66.36%, Test: 38.87%\n",
      "learning rate: 0.0012037\n",
      ": Hits@20: Epoch: 169, Loss: 0.1619, Valid: 65.84%, Test: 22.32%\n",
      "learning rate: 0.0012001\n",
      ": Hits@20: Epoch: 170, Loss: 0.1618, Valid: 66.11%, Test: 34.63%\n",
      "learning rate: 0.0011965\n",
      ": Hits@20: Epoch: 171, Loss: 0.1611, Valid: 67.68%, Test: 35.78%\n",
      "learning rate: 0.0011929\n",
      ": Hits@20: Epoch: 172, Loss: 0.1614, Valid: 64.20%, Test: 37.64%\n",
      "learning rate: 0.0011893\n",
      ": Hits@20: Epoch: 173, Loss: 0.1612, Valid: 67.06%, Test: 12.54%\n",
      "learning rate: 0.0011857\n",
      ": Hits@20: Epoch: 174, Loss: 0.1600, Valid: 65.79%, Test: 22.26%\n",
      "learning rate: 0.0011822\n",
      ": Hits@20: Epoch: 175, Loss: 0.1602, Valid: 64.48%, Test: 12.71%\n",
      "learning rate: 0.0011786\n",
      ": Hits@20: Epoch: 176, Loss: 0.1584, Valid: 67.11%, Test: 20.34%\n",
      "learning rate: 0.0011751\n",
      ": Hits@20: Epoch: 177, Loss: 0.1613, Valid: 67.23%, Test: 17.22%\n",
      "learning rate: 0.0011716\n",
      ": Hits@20: Epoch: 178, Loss: 0.1591, Valid: 66.90%, Test: 22.57%\n",
      "learning rate: 0.0011681\n",
      ": Hits@20: Epoch: 179, Loss: 0.1595, Valid: 66.21%, Test: 26.37%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0011646\n",
      ": Hits@20: Epoch: 180, Loss: 0.1593, Valid: 66.08%, Test: 10.55%\n",
      "learning rate: 0.0011611\n",
      ": Hits@20: Epoch: 181, Loss: 0.1585, Valid: 67.82%, Test: 31.09%\n",
      "learning rate: 0.0011576\n",
      ": Hits@20: Epoch: 182, Loss: 0.1588, Valid: 66.51%, Test: 48.12%\n",
      "learning rate: 0.0011541\n",
      ": Hits@20: Epoch: 183, Loss: 0.1574, Valid: 67.82%, Test: 28.88%\n",
      "learning rate: 0.0011506\n",
      ": Hits@20: Epoch: 184, Loss: 0.1577, Valid: 67.76%, Test: 24.57%\n",
      "learning rate: 0.0011472\n",
      ": Hits@20: Epoch: 185, Loss: 0.1580, Valid: 67.64%, Test: 23.76%\n",
      "learning rate: 0.0011437\n",
      ": Hits@20: Epoch: 186, Loss: 0.1569, Valid: 68.06%, Test: 57.10%\n",
      "learning rate: 0.0011403\n",
      ": Hits@20: Epoch: 187, Loss: 0.1564, Valid: 67.62%, Test: 42.76%\n",
      "learning rate: 0.0011369\n",
      ": Hits@20: Epoch: 188, Loss: 0.1574, Valid: 68.17%, Test: 31.08%\n",
      "learning rate: 0.0011335\n",
      ": Hits@20: Epoch: 189, Loss: 0.1569, Valid: 69.22%, Test: 46.31%\n",
      "learning rate: 0.0011301\n",
      ": Hits@20: Epoch: 190, Loss: 0.1567, Valid: 68.10%, Test: 26.98%\n",
      "learning rate: 0.0011267\n",
      ": Hits@20: Epoch: 191, Loss: 0.1570, Valid: 67.41%, Test: 44.99%\n",
      "learning rate: 0.0011233\n",
      ": Hits@20: Epoch: 192, Loss: 0.1562, Valid: 66.72%, Test: 44.27%\n",
      "learning rate: 0.0011199\n",
      ": Hits@20: Epoch: 193, Loss: 0.1558, Valid: 67.72%, Test: 62.90%\n",
      "learning rate: 0.0011166\n",
      ": Hits@20: Epoch: 194, Loss: 0.1564, Valid: 65.94%, Test: 44.27%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0011132\n",
      ": Hits@20: Epoch: 195, Loss: 0.1558, Valid: 68.14%, Test: 30.72%\n",
      "learning rate: 0.0011099\n",
      ": Hits@20: Epoch: 196, Loss: 0.1559, Valid: 67.82%, Test: 28.02%\n",
      "learning rate: 0.0011066\n",
      ": Hits@20: Epoch: 197, Loss: 0.1551, Valid: 67.55%, Test: 30.95%\n",
      "learning rate: 0.0011032\n",
      ": Hits@20: Epoch: 198, Loss: 0.1550, Valid: 67.24%, Test: 9.45%\n",
      "learning rate: 0.0010999\n",
      ": Hits@20: Epoch: 199, Loss: 0.1550, Valid: 67.39%, Test: 13.32%\n",
      "learning rate: 0.0010966\n",
      ": Hits@20: Epoch: 200, Loss: 0.1532, Valid: 69.20%, Test: 31.62%\n",
      "learning rate: 0.0010933\n",
      ": Hits@20: Epoch: 201, Loss: 0.1541, Valid: 68.32%, Test: 23.07%\n",
      "learning rate: 0.0010901\n",
      ": Hits@20: Epoch: 202, Loss: 0.1539, Valid: 68.03%, Test: 47.99%\n",
      "learning rate: 0.0010868\n",
      ": Hits@20: Epoch: 203, Loss: 0.1545, Valid: 69.20%, Test: 45.68%\n",
      "learning rate: 0.0010835\n",
      ": Hits@20: Epoch: 204, Loss: 0.1536, Valid: 69.53%, Test: 23.44%\n",
      "learning rate: 0.0010803\n",
      ": Hits@20: Epoch: 205, Loss: 0.1545, Valid: 68.76%, Test: 43.97%\n",
      "learning rate: 0.0010770\n",
      ": Hits@20: Epoch: 206, Loss: 0.1527, Valid: 69.37%, Test: 46.37%\n",
      "learning rate: 0.0010738\n",
      ": Hits@20: Epoch: 207, Loss: 0.1542, Valid: 67.57%, Test: 12.23%\n",
      "learning rate: 0.0010706\n",
      ": Hits@20: Epoch: 208, Loss: 0.1531, Valid: 68.69%, Test: 31.94%\n",
      "learning rate: 0.0010674\n",
      ": Hits@20: Epoch: 209, Loss: 0.1530, Valid: 68.85%, Test: 7.02%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0010642\n",
      ": Hits@20: Epoch: 210, Loss: 0.1534, Valid: 69.81%, Test: 32.12%\n",
      "learning rate: 0.0010610\n",
      ": Hits@20: Epoch: 211, Loss: 0.1527, Valid: 68.76%, Test: 15.33%\n",
      "learning rate: 0.0010578\n",
      ": Hits@20: Epoch: 212, Loss: 0.1524, Valid: 68.80%, Test: 47.26%\n",
      "learning rate: 0.0010546\n",
      ": Hits@20: Epoch: 213, Loss: 0.1529, Valid: 69.26%, Test: 32.02%\n",
      "learning rate: 0.0010515\n",
      ": Hits@20: Epoch: 214, Loss: 0.1532, Valid: 70.39%, Test: 37.77%\n",
      "learning rate: 0.0010483\n",
      ": Hits@20: Epoch: 215, Loss: 0.1525, Valid: 70.54%, Test: 9.51%\n",
      "learning rate: 0.0010452\n",
      ": Hits@20: Epoch: 216, Loss: 0.1525, Valid: 70.11%, Test: 26.65%\n",
      "learning rate: 0.0010420\n",
      ": Hits@20: Epoch: 217, Loss: 0.1529, Valid: 70.03%, Test: 14.67%\n",
      "learning rate: 0.0010389\n",
      ": Hits@20: Epoch: 218, Loss: 0.1525, Valid: 70.72%, Test: 8.86%\n",
      "learning rate: 0.0010358\n",
      ": Hits@20: Epoch: 219, Loss: 0.1508, Valid: 69.71%, Test: 30.18%\n",
      "learning rate: 0.0010327\n",
      ": Hits@20: Epoch: 220, Loss: 0.1514, Valid: 70.39%, Test: 60.43%\n",
      "learning rate: 0.0010296\n",
      ": Hits@20: Epoch: 221, Loss: 0.1515, Valid: 70.03%, Test: 26.76%\n",
      "learning rate: 0.0010265\n",
      ": Hits@20: Epoch: 222, Loss: 0.1506, Valid: 70.22%, Test: 51.73%\n",
      "learning rate: 0.0010234\n",
      ": Hits@20: Epoch: 223, Loss: 0.1507, Valid: 70.18%, Test: 43.89%\n",
      "learning rate: 0.0010203\n",
      ": Hits@20: Epoch: 224, Loss: 0.1527, Valid: 70.34%, Test: 27.94%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0010173\n",
      ": Hits@20: Epoch: 225, Loss: 0.1513, Valid: 70.01%, Test: 41.88%\n",
      "learning rate: 0.0010142\n",
      ": Hits@20: Epoch: 226, Loss: 0.1502, Valid: 70.70%, Test: 47.91%\n",
      "learning rate: 0.0010112\n",
      ": Hits@20: Epoch: 227, Loss: 0.1498, Valid: 70.10%, Test: 21.85%\n",
      "learning rate: 0.0010082\n",
      ": Hits@20: Epoch: 228, Loss: 0.1515, Valid: 69.94%, Test: 73.43%\n",
      "learning rate: 0.0010051\n",
      ": Hits@20: Epoch: 229, Loss: 0.1504, Valid: 70.42%, Test: 18.39%\n",
      "learning rate: 0.0010021\n",
      ": Hits@20: Epoch: 230, Loss: 0.1499, Valid: 70.03%, Test: 45.44%\n",
      "learning rate: 0.0009991\n",
      ": Hits@20: Epoch: 231, Loss: 0.1504, Valid: 70.13%, Test: 48.84%\n",
      "learning rate: 0.0009961\n",
      ": Hits@20: Epoch: 232, Loss: 0.1498, Valid: 69.06%, Test: 29.72%\n",
      "learning rate: 0.0009931\n",
      ": Hits@20: Epoch: 233, Loss: 0.1491, Valid: 70.54%, Test: 44.71%\n",
      "learning rate: 0.0009901\n",
      ": Hits@20: Epoch: 234, Loss: 0.1495, Valid: 70.87%, Test: 26.90%\n",
      "learning rate: 0.0009872\n",
      ": Hits@20: Epoch: 235, Loss: 0.1492, Valid: 71.33%, Test: 25.16%\n",
      "learning rate: 0.0009842\n",
      ": Hits@20: Epoch: 236, Loss: 0.1494, Valid: 70.80%, Test: 24.95%\n",
      "learning rate: 0.0009813\n",
      ": Hits@20: Epoch: 237, Loss: 0.1501, Valid: 70.99%, Test: 45.01%\n",
      "learning rate: 0.0009783\n",
      ": Hits@20: Epoch: 238, Loss: 0.1501, Valid: 70.42%, Test: 33.06%\n",
      "learning rate: 0.0009754\n",
      ": Hits@20: Epoch: 239, Loss: 0.1488, Valid: 71.72%, Test: 25.55%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0009725\n",
      ": Hits@20: Epoch: 240, Loss: 0.1486, Valid: 70.23%, Test: 62.10%\n",
      "learning rate: 0.0009695\n",
      ": Hits@20: Epoch: 241, Loss: 0.1497, Valid: 70.68%, Test: 52.80%\n",
      "learning rate: 0.0009666\n",
      ": Hits@20: Epoch: 242, Loss: 0.1469, Valid: 70.81%, Test: 52.24%\n",
      "learning rate: 0.0009637\n",
      ": Hits@20: Epoch: 243, Loss: 0.1489, Valid: 70.68%, Test: 62.49%\n",
      "learning rate: 0.0009608\n",
      ": Hits@20: Epoch: 244, Loss: 0.1492, Valid: 70.85%, Test: 12.78%\n",
      "learning rate: 0.0009580\n",
      ": Hits@20: Epoch: 245, Loss: 0.1483, Valid: 71.10%, Test: 24.90%\n",
      "learning rate: 0.0009551\n",
      ": Hits@20: Epoch: 246, Loss: 0.1486, Valid: 70.85%, Test: 37.05%\n",
      "learning rate: 0.0009522\n",
      ": Hits@20: Epoch: 247, Loss: 0.1485, Valid: 70.36%, Test: 19.98%\n",
      "learning rate: 0.0009494\n",
      ": Hits@20: Epoch: 248, Loss: 0.1487, Valid: 70.84%, Test: 32.13%\n",
      "learning rate: 0.0009465\n",
      ": Hits@20: Epoch: 249, Loss: 0.1487, Valid: 70.68%, Test: 47.56%\n",
      "learning rate: 0.0009437\n",
      ": Hits@20: Epoch: 250, Loss: 0.1481, Valid: 71.66%, Test: 61.32%\n",
      "learning rate: 0.0009408\n",
      ": Hits@20: Epoch: 251, Loss: 0.1476, Valid: 70.90%, Test: 28.31%\n",
      "learning rate: 0.0009380\n",
      ": Hits@20: Epoch: 252, Loss: 0.1473, Valid: 71.59%, Test: 28.14%\n",
      "learning rate: 0.0009352\n",
      ": Hits@20: Epoch: 253, Loss: 0.1482, Valid: 71.36%, Test: 44.98%\n",
      "learning rate: 0.0009324\n",
      ": Hits@20: Epoch: 254, Loss: 0.1478, Valid: 71.75%, Test: 34.45%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0009296\n",
      ": Hits@20: Epoch: 255, Loss: 0.1471, Valid: 71.96%, Test: 38.20%\n",
      "learning rate: 0.0009268\n",
      ": Hits@20: Epoch: 256, Loss: 0.1483, Valid: 71.15%, Test: 68.72%\n",
      "learning rate: 0.0009240\n",
      ": Hits@20: Epoch: 257, Loss: 0.1473, Valid: 71.00%, Test: 69.95%\n",
      "learning rate: 0.0009213\n",
      ": Hits@20: Epoch: 258, Loss: 0.1470, Valid: 71.26%, Test: 79.22%\n",
      "learning rate: 0.0009185\n",
      ": Hits@20: Epoch: 259, Loss: 0.1457, Valid: 71.00%, Test: 67.58%\n",
      "learning rate: 0.0009157\n",
      ": Hits@20: Epoch: 260, Loss: 0.1469, Valid: 70.48%, Test: 46.68%\n",
      "learning rate: 0.0009130\n",
      ": Hits@20: Epoch: 261, Loss: 0.1469, Valid: 71.78%, Test: 70.96%\n",
      "learning rate: 0.0009103\n",
      ": Hits@20: Epoch: 262, Loss: 0.1474, Valid: 70.87%, Test: 27.47%\n",
      "learning rate: 0.0009075\n",
      ": Hits@20: Epoch: 263, Loss: 0.1466, Valid: 71.15%, Test: 29.12%\n",
      "learning rate: 0.0009048\n",
      ": Hits@20: Epoch: 264, Loss: 0.1473, Valid: 71.22%, Test: 14.47%\n",
      "learning rate: 0.0009021\n",
      ": Hits@20: Epoch: 265, Loss: 0.1465, Valid: 71.84%, Test: 16.69%\n",
      "learning rate: 0.0008994\n",
      ": Hits@20: Epoch: 266, Loss: 0.1461, Valid: 71.48%, Test: 19.80%\n",
      "learning rate: 0.0008967\n",
      ": Hits@20: Epoch: 267, Loss: 0.1465, Valid: 70.95%, Test: 40.00%\n",
      "learning rate: 0.0008940\n",
      ": Hits@20: Epoch: 268, Loss: 0.1485, Valid: 71.88%, Test: 13.94%\n",
      "learning rate: 0.0008913\n",
      ": Hits@20: Epoch: 269, Loss: 0.1458, Valid: 71.70%, Test: 57.72%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0008886\n",
      ": Hits@20: Epoch: 270, Loss: 0.1460, Valid: 71.06%, Test: 26.73%\n",
      "learning rate: 0.0008860\n",
      ": Hits@20: Epoch: 271, Loss: 0.1456, Valid: 71.51%, Test: 77.95%\n",
      "learning rate: 0.0008833\n",
      ": Hits@20: Epoch: 272, Loss: 0.1457, Valid: 71.16%, Test: 32.68%\n",
      "learning rate: 0.0008807\n",
      ": Hits@20: Epoch: 273, Loss: 0.1463, Valid: 71.70%, Test: 76.98%\n",
      "learning rate: 0.0008780\n",
      ": Hits@20: Epoch: 274, Loss: 0.1461, Valid: 71.23%, Test: 37.34%\n",
      "learning rate: 0.0008754\n",
      ": Hits@20: Epoch: 275, Loss: 0.1452, Valid: 71.52%, Test: 23.07%\n",
      "learning rate: 0.0008728\n",
      ": Hits@20: Epoch: 276, Loss: 0.1461, Valid: 71.41%, Test: 35.47%\n",
      "learning rate: 0.0008701\n",
      ": Hits@20: Epoch: 277, Loss: 0.1457, Valid: 71.26%, Test: 72.48%\n",
      "learning rate: 0.0008675\n",
      ": Hits@20: Epoch: 278, Loss: 0.1462, Valid: 71.72%, Test: 25.41%\n",
      "learning rate: 0.0008649\n",
      ": Hits@20: Epoch: 279, Loss: 0.1456, Valid: 71.33%, Test: 78.58%\n",
      "learning rate: 0.0008623\n",
      ": Hits@20: Epoch: 280, Loss: 0.1453, Valid: 71.50%, Test: 64.72%\n",
      "learning rate: 0.0008597\n",
      ": Hits@20: Epoch: 281, Loss: 0.1461, Valid: 71.85%, Test: 85.98%\n",
      "learning rate: 0.0008572\n",
      ": Hits@20: Epoch: 282, Loss: 0.1459, Valid: 71.83%, Test: 84.50%\n",
      "learning rate: 0.0008546\n",
      ": Hits@20: Epoch: 283, Loss: 0.1448, Valid: 71.08%, Test: 76.68%\n",
      "learning rate: 0.0008520\n",
      ": Hits@20: Epoch: 284, Loss: 0.1459, Valid: 71.80%, Test: 90.14%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0008495\n",
      ": Hits@20: Epoch: 285, Loss: 0.1458, Valid: 72.46%, Test: 72.24%\n",
      "learning rate: 0.0008469\n",
      ": Hits@20: Epoch: 286, Loss: 0.1461, Valid: 71.67%, Test: 61.43%\n",
      "learning rate: 0.0008444\n",
      ": Hits@20: Epoch: 287, Loss: 0.1449, Valid: 72.10%, Test: 70.49%\n",
      "learning rate: 0.0008419\n",
      ": Hits@20: Epoch: 288, Loss: 0.1443, Valid: 72.47%, Test: 65.39%\n",
      "learning rate: 0.0008393\n",
      ": Hits@20: Epoch: 289, Loss: 0.1445, Valid: 72.19%, Test: 75.69%\n",
      "learning rate: 0.0008368\n",
      ": Hits@20: Epoch: 290, Loss: 0.1443, Valid: 72.59%, Test: 87.57%\n",
      "learning rate: 0.0008343\n",
      ": Hits@20: Epoch: 291, Loss: 0.1449, Valid: 72.09%, Test: 76.26%\n",
      "learning rate: 0.0008318\n",
      ": Hits@20: Epoch: 292, Loss: 0.1441, Valid: 72.17%, Test: 80.54%\n",
      "learning rate: 0.0008293\n",
      ": Hits@20: Epoch: 293, Loss: 0.1450, Valid: 72.36%, Test: 47.35%\n",
      "learning rate: 0.0008268\n",
      ": Hits@20: Epoch: 294, Loss: 0.1436, Valid: 72.56%, Test: 66.39%\n",
      "learning rate: 0.0008243\n",
      ": Hits@20: Epoch: 295, Loss: 0.1431, Valid: 72.19%, Test: 54.94%\n",
      "learning rate: 0.0008219\n",
      ": Hits@20: Epoch: 296, Loss: 0.1433, Valid: 71.38%, Test: 39.84%\n",
      "learning rate: 0.0008194\n",
      ": Hits@20: Epoch: 297, Loss: 0.1442, Valid: 72.34%, Test: 68.27%\n",
      "learning rate: 0.0008169\n",
      ": Hits@20: Epoch: 298, Loss: 0.1444, Valid: 72.18%, Test: 49.66%\n",
      "learning rate: 0.0008145\n",
      ": Hits@20: Epoch: 299, Loss: 0.1448, Valid: 71.87%, Test: 77.57%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0008120\n",
      ": Hits@20: Epoch: 300, Loss: 0.1441, Valid: 72.33%, Test: 43.00%\n",
      "learning rate: 0.0008096\n",
      ": Hits@20: Epoch: 301, Loss: 0.1437, Valid: 72.44%, Test: 64.07%\n",
      "learning rate: 0.0008072\n",
      ": Hits@20: Epoch: 302, Loss: 0.1443, Valid: 72.62%, Test: 75.02%\n",
      "learning rate: 0.0008048\n",
      ": Hits@20: Epoch: 303, Loss: 0.1434, Valid: 71.90%, Test: 63.62%\n",
      "learning rate: 0.0008023\n",
      ": Hits@20: Epoch: 304, Loss: 0.1445, Valid: 73.02%, Test: 80.26%\n",
      "learning rate: 0.0007999\n",
      ": Hits@20: Epoch: 305, Loss: 0.1443, Valid: 72.64%, Test: 85.44%\n",
      "learning rate: 0.0007975\n",
      ": Hits@20: Epoch: 306, Loss: 0.1432, Valid: 72.15%, Test: 85.74%\n",
      "learning rate: 0.0007951\n",
      ": Hits@20: Epoch: 307, Loss: 0.1430, Valid: 72.49%, Test: 88.52%\n",
      "learning rate: 0.0007928\n",
      ": Hits@20: Epoch: 308, Loss: 0.1430, Valid: 72.63%, Test: 54.12%\n",
      "learning rate: 0.0007904\n",
      ": Hits@20: Epoch: 309, Loss: 0.1441, Valid: 72.75%, Test: 28.63%\n",
      "learning rate: 0.0007880\n",
      ": Hits@20: Epoch: 310, Loss: 0.1435, Valid: 72.63%, Test: 57.75%\n",
      "learning rate: 0.0007856\n",
      ": Hits@20: Epoch: 311, Loss: 0.1426, Valid: 72.91%, Test: 82.60%\n",
      "learning rate: 0.0007833\n",
      ": Hits@20: Epoch: 312, Loss: 0.1438, Valid: 72.19%, Test: 37.35%\n",
      "learning rate: 0.0007809\n",
      ": Hits@20: Epoch: 313, Loss: 0.1436, Valid: 72.67%, Test: 75.87%\n",
      "learning rate: 0.0007786\n",
      ": Hits@20: Epoch: 314, Loss: 0.1432, Valid: 72.83%, Test: 55.41%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0007763\n",
      ": Hits@20: Epoch: 315, Loss: 0.1416, Valid: 72.98%, Test: 45.18%\n",
      "learning rate: 0.0007739\n",
      ": Hits@20: Epoch: 316, Loss: 0.1426, Valid: 72.54%, Test: 24.42%\n",
      "learning rate: 0.0007716\n",
      ": Hits@20: Epoch: 317, Loss: 0.1432, Valid: 72.34%, Test: 22.70%\n",
      "learning rate: 0.0007693\n",
      ": Hits@20: Epoch: 318, Loss: 0.1430, Valid: 72.59%, Test: 24.66%\n",
      "learning rate: 0.0007670\n",
      ": Hits@20: Epoch: 319, Loss: 0.1430, Valid: 72.80%, Test: 43.68%\n",
      "learning rate: 0.0007647\n",
      ": Hits@20: Epoch: 320, Loss: 0.1438, Valid: 72.21%, Test: 83.97%\n",
      "learning rate: 0.0007624\n",
      ": Hits@20: Epoch: 321, Loss: 0.1421, Valid: 72.41%, Test: 69.15%\n",
      "learning rate: 0.0007601\n",
      ": Hits@20: Epoch: 322, Loss: 0.1427, Valid: 72.77%, Test: 81.09%\n",
      "learning rate: 0.0007578\n",
      ": Hits@20: Epoch: 323, Loss: 0.1425, Valid: 72.56%, Test: 81.17%\n",
      "learning rate: 0.0007555\n",
      ": Hits@20: Epoch: 324, Loss: 0.1429, Valid: 71.84%, Test: 77.19%\n",
      "learning rate: 0.0007533\n",
      ": Hits@20: Epoch: 325, Loss: 0.1418, Valid: 72.68%, Test: 73.69%\n",
      "learning rate: 0.0007510\n",
      ": Hits@20: Epoch: 326, Loss: 0.1422, Valid: 72.50%, Test: 65.73%\n",
      "learning rate: 0.0007488\n",
      ": Hits@20: Epoch: 327, Loss: 0.1433, Valid: 72.55%, Test: 66.97%\n",
      "learning rate: 0.0007465\n",
      ": Hits@20: Epoch: 328, Loss: 0.1429, Valid: 72.35%, Test: 86.57%\n",
      "learning rate: 0.0007443\n",
      ": Hits@20: Epoch: 329, Loss: 0.1421, Valid: 72.81%, Test: 66.89%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0007420\n",
      ": Hits@20: Epoch: 330, Loss: 0.1427, Valid: 72.68%, Test: 55.19%\n",
      "learning rate: 0.0007398\n",
      ": Hits@20: Epoch: 331, Loss: 0.1424, Valid: 73.03%, Test: 70.06%\n",
      "learning rate: 0.0007376\n",
      ": Hits@20: Epoch: 332, Loss: 0.1427, Valid: 72.93%, Test: 79.98%\n",
      "learning rate: 0.0007354\n",
      ": Hits@20: Epoch: 333, Loss: 0.1425, Valid: 72.89%, Test: 89.54%\n",
      "learning rate: 0.0007332\n",
      ": Hits@20: Epoch: 334, Loss: 0.1419, Valid: 72.63%, Test: 84.42%\n",
      "learning rate: 0.0007310\n",
      ": Hits@20: Epoch: 335, Loss: 0.1432, Valid: 72.70%, Test: 79.71%\n",
      "learning rate: 0.0007288\n",
      ": Hits@20: Epoch: 336, Loss: 0.1411, Valid: 72.36%, Test: 41.18%\n",
      "learning rate: 0.0007266\n",
      ": Hits@20: Epoch: 337, Loss: 0.1420, Valid: 72.93%, Test: 53.59%\n",
      "learning rate: 0.0007244\n",
      ": Hits@20: Epoch: 338, Loss: 0.1415, Valid: 72.56%, Test: 74.09%\n",
      "learning rate: 0.0007223\n",
      ": Hits@20: Epoch: 339, Loss: 0.1411, Valid: 72.45%, Test: 40.72%\n",
      "learning rate: 0.0007201\n",
      ": Hits@20: Epoch: 340, Loss: 0.1409, Valid: 72.50%, Test: 62.58%\n",
      "learning rate: 0.0007179\n",
      ": Hits@20: Epoch: 341, Loss: 0.1428, Valid: 72.39%, Test: 85.45%\n",
      "learning rate: 0.0007158\n",
      ": Hits@20: Epoch: 342, Loss: 0.1415, Valid: 72.33%, Test: 71.97%\n",
      "learning rate: 0.0007136\n",
      ": Hits@20: Epoch: 343, Loss: 0.1409, Valid: 72.51%, Test: 80.24%\n",
      "learning rate: 0.0007115\n",
      ": Hits@20: Epoch: 344, Loss: 0.1416, Valid: 72.63%, Test: 60.31%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0007093\n",
      ": Hits@20: Epoch: 345, Loss: 0.1407, Valid: 72.57%, Test: 36.69%\n",
      "learning rate: 0.0007072\n",
      ": Hits@20: Epoch: 346, Loss: 0.1417, Valid: 72.94%, Test: 81.02%\n",
      "learning rate: 0.0007051\n",
      ": Hits@20: Epoch: 347, Loss: 0.1406, Valid: 72.61%, Test: 55.31%\n",
      "learning rate: 0.0007030\n",
      ": Hits@20: Epoch: 348, Loss: 0.1411, Valid: 72.68%, Test: 85.51%\n",
      "learning rate: 0.0007009\n",
      ": Hits@20: Epoch: 349, Loss: 0.1402, Valid: 72.60%, Test: 80.16%\n",
      "learning rate: 0.0006988\n",
      ": Hits@20: Epoch: 350, Loss: 0.1414, Valid: 72.78%, Test: 91.91%\n",
      "learning rate: 0.0006967\n",
      ": Hits@20: Epoch: 351, Loss: 0.1425, Valid: 72.73%, Test: 92.67%\n",
      "learning rate: 0.0006946\n",
      ": Hits@20: Epoch: 352, Loss: 0.1415, Valid: 73.17%, Test: 82.46%\n",
      "learning rate: 0.0006925\n",
      ": Hits@20: Epoch: 353, Loss: 0.1417, Valid: 73.04%, Test: 71.39%\n",
      "learning rate: 0.0006904\n",
      ": Hits@20: Epoch: 354, Loss: 0.1408, Valid: 72.96%, Test: 87.48%\n",
      "learning rate: 0.0006884\n",
      ": Hits@20: Epoch: 355, Loss: 0.1420, Valid: 72.79%, Test: 44.97%\n",
      "learning rate: 0.0006863\n",
      ": Hits@20: Epoch: 356, Loss: 0.1402, Valid: 72.72%, Test: 71.04%\n",
      "learning rate: 0.0006842\n",
      ": Hits@20: Epoch: 357, Loss: 0.1402, Valid: 72.39%, Test: 64.60%\n",
      "learning rate: 0.0006822\n",
      ": Hits@20: Epoch: 358, Loss: 0.1408, Valid: 72.79%, Test: 57.34%\n",
      "learning rate: 0.0006801\n",
      ": Hits@20: Epoch: 359, Loss: 0.1410, Valid: 72.83%, Test: 55.53%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0006781\n",
      ": Hits@20: Epoch: 360, Loss: 0.1408, Valid: 73.05%, Test: 55.78%\n",
      "learning rate: 0.0006761\n",
      ": Hits@20: Epoch: 361, Loss: 0.1401, Valid: 73.12%, Test: 72.93%\n",
      "learning rate: 0.0006740\n",
      ": Hits@20: Epoch: 362, Loss: 0.1421, Valid: 73.03%, Test: 86.49%\n",
      "learning rate: 0.0006720\n",
      ": Hits@20: Epoch: 363, Loss: 0.1405, Valid: 73.09%, Test: 55.58%\n",
      "learning rate: 0.0006700\n",
      ": Hits@20: Epoch: 364, Loss: 0.1401, Valid: 73.28%, Test: 46.88%\n",
      "learning rate: 0.0006680\n",
      ": Hits@20: Epoch: 365, Loss: 0.1407, Valid: 72.66%, Test: 41.95%\n",
      "learning rate: 0.0006660\n",
      ": Hits@20: Epoch: 366, Loss: 0.1411, Valid: 72.59%, Test: 68.06%\n",
      "learning rate: 0.0006640\n",
      ": Hits@20: Epoch: 367, Loss: 0.1401, Valid: 72.51%, Test: 69.10%\n",
      "learning rate: 0.0006620\n",
      ": Hits@20: Epoch: 368, Loss: 0.1396, Valid: 72.77%, Test: 49.99%\n",
      "learning rate: 0.0006600\n",
      ": Hits@20: Epoch: 369, Loss: 0.1404, Valid: 72.91%, Test: 55.10%\n",
      "learning rate: 0.0006580\n",
      ": Hits@20: Epoch: 370, Loss: 0.1413, Valid: 73.24%, Test: 79.79%\n",
      "learning rate: 0.0006560\n",
      ": Hits@20: Epoch: 371, Loss: 0.1399, Valid: 73.36%, Test: 53.27%\n",
      "learning rate: 0.0006541\n",
      ": Hits@20: Epoch: 372, Loss: 0.1397, Valid: 72.83%, Test: 49.27%\n",
      "learning rate: 0.0006521\n",
      ": Hits@20: Epoch: 373, Loss: 0.1404, Valid: 72.85%, Test: 74.96%\n",
      "learning rate: 0.0006502\n",
      ": Hits@20: Epoch: 374, Loss: 0.1399, Valid: 72.77%, Test: 67.74%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0006482\n",
      ": Hits@20: Epoch: 375, Loss: 0.1375, Valid: 72.55%, Test: 58.54%\n",
      "learning rate: 0.0006463\n",
      ": Hits@20: Epoch: 376, Loss: 0.1401, Valid: 72.68%, Test: 84.63%\n",
      "learning rate: 0.0006443\n",
      ": Hits@20: Epoch: 377, Loss: 0.1400, Valid: 72.92%, Test: 46.02%\n",
      "learning rate: 0.0006424\n",
      ": Hits@20: Epoch: 378, Loss: 0.1391, Valid: 73.01%, Test: 82.79%\n",
      "learning rate: 0.0006405\n",
      ": Hits@20: Epoch: 379, Loss: 0.1398, Valid: 73.18%, Test: 87.80%\n",
      "learning rate: 0.0006385\n",
      ": Hits@20: Epoch: 380, Loss: 0.1390, Valid: 73.43%, Test: 92.40%\n",
      "learning rate: 0.0006366\n",
      ": Hits@20: Epoch: 381, Loss: 0.1400, Valid: 73.49%, Test: 79.40%\n",
      "learning rate: 0.0006347\n",
      ": Hits@20: Epoch: 382, Loss: 0.1410, Valid: 73.34%, Test: 84.76%\n",
      "learning rate: 0.0006328\n",
      ": Hits@20: Epoch: 383, Loss: 0.1392, Valid: 73.27%, Test: 59.20%\n",
      "learning rate: 0.0006309\n",
      ": Hits@20: Epoch: 384, Loss: 0.1401, Valid: 73.14%, Test: 49.20%\n",
      "learning rate: 0.0006290\n",
      ": Hits@20: Epoch: 385, Loss: 0.1398, Valid: 73.26%, Test: 52.86%\n",
      "learning rate: 0.0006271\n",
      ": Hits@20: Epoch: 386, Loss: 0.1392, Valid: 73.39%, Test: 87.47%\n",
      "learning rate: 0.0006253\n",
      ": Hits@20: Epoch: 387, Loss: 0.1402, Valid: 73.13%, Test: 42.23%\n",
      "learning rate: 0.0006234\n",
      ": Hits@20: Epoch: 388, Loss: 0.1399, Valid: 73.34%, Test: 84.80%\n",
      "learning rate: 0.0006215\n",
      ": Hits@20: Epoch: 389, Loss: 0.1394, Valid: 73.02%, Test: 60.01%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0006196\n",
      ": Hits@20: Epoch: 390, Loss: 0.1383, Valid: 73.13%, Test: 79.80%\n",
      "learning rate: 0.0006178\n",
      ": Hits@20: Epoch: 391, Loss: 0.1393, Valid: 72.94%, Test: 90.03%\n",
      "learning rate: 0.0006159\n",
      ": Hits@20: Epoch: 392, Loss: 0.1397, Valid: 73.30%, Test: 93.00%\n",
      "learning rate: 0.0006141\n",
      ": Hits@20: Epoch: 393, Loss: 0.1390, Valid: 73.02%, Test: 80.31%\n",
      "learning rate: 0.0006122\n",
      ": Hits@20: Epoch: 394, Loss: 0.1395, Valid: 73.30%, Test: 61.56%\n",
      "learning rate: 0.0006104\n",
      ": Hits@20: Epoch: 395, Loss: 0.1392, Valid: 73.41%, Test: 77.67%\n",
      "learning rate: 0.0006086\n",
      ": Hits@20: Epoch: 396, Loss: 0.1381, Valid: 73.67%, Test: 87.19%\n",
      "learning rate: 0.0006067\n",
      ": Hits@20: Epoch: 397, Loss: 0.1391, Valid: 73.65%, Test: 73.17%\n",
      "learning rate: 0.0006049\n",
      ": Hits@20: Epoch: 398, Loss: 0.1389, Valid: 73.42%, Test: 66.56%\n",
      "learning rate: 0.0006031\n",
      ": Hits@20: Epoch: 399, Loss: 0.1388, Valid: 73.47%, Test: 94.31%\n",
      "learning rate: 0.0006013\n",
      ": Hits@20: Epoch: 400, Loss: 0.1387, Valid: 73.34%, Test: 85.60%\n",
      "learning rate: 0.0005995\n",
      ": Hits@20: Epoch: 401, Loss: 0.1393, Valid: 73.15%, Test: 72.90%\n",
      "learning rate: 0.0005977\n",
      ": Hits@20: Epoch: 402, Loss: 0.1402, Valid: 72.94%, Test: 94.07%\n",
      "learning rate: 0.0005959\n",
      ": Hits@20: Epoch: 403, Loss: 0.1390, Valid: 73.43%, Test: 82.87%\n",
      "learning rate: 0.0005941\n",
      ": Hits@20: Epoch: 404, Loss: 0.1393, Valid: 73.34%, Test: 85.15%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0005923\n",
      ": Hits@20: Epoch: 405, Loss: 0.1390, Valid: 73.40%, Test: 85.63%\n",
      "learning rate: 0.0005906\n",
      ": Hits@20: Epoch: 406, Loss: 0.1381, Valid: 73.63%, Test: 94.43%\n",
      "learning rate: 0.0005888\n",
      ": Hits@20: Epoch: 407, Loss: 0.1381, Valid: 73.47%, Test: 84.28%\n",
      "learning rate: 0.0005870\n",
      ": Hits@20: Epoch: 408, Loss: 0.1380, Valid: 73.26%, Test: 94.15%\n",
      "learning rate: 0.0005853\n",
      ": Hits@20: Epoch: 409, Loss: 0.1383, Valid: 73.52%, Test: 93.79%\n",
      "learning rate: 0.0005835\n",
      ": Hits@20: Epoch: 410, Loss: 0.1389, Valid: 73.40%, Test: 60.21%\n",
      "learning rate: 0.0005818\n",
      ": Hits@20: Epoch: 411, Loss: 0.1398, Valid: 73.43%, Test: 74.03%\n",
      "learning rate: 0.0005800\n",
      ": Hits@20: Epoch: 412, Loss: 0.1387, Valid: 73.33%, Test: 90.61%\n",
      "learning rate: 0.0005783\n",
      ": Hits@20: Epoch: 413, Loss: 0.1390, Valid: 73.12%, Test: 76.13%\n",
      "learning rate: 0.0005765\n",
      ": Hits@20: Epoch: 414, Loss: 0.1392, Valid: 73.24%, Test: 92.11%\n",
      "learning rate: 0.0005748\n",
      ": Hits@20: Epoch: 415, Loss: 0.1380, Valid: 72.94%, Test: 67.06%\n",
      "learning rate: 0.0005731\n",
      ": Hits@20: Epoch: 416, Loss: 0.1378, Valid: 73.32%, Test: 73.67%\n",
      "learning rate: 0.0005714\n",
      ": Hits@20: Epoch: 417, Loss: 0.1383, Valid: 72.96%, Test: 83.28%\n",
      "learning rate: 0.0005696\n",
      ": Hits@20: Epoch: 418, Loss: 0.1378, Valid: 73.37%, Test: 83.64%\n",
      "learning rate: 0.0005679\n",
      ": Hits@20: Epoch: 419, Loss: 0.1397, Valid: 73.35%, Test: 89.56%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0005662\n",
      ": Hits@20: Epoch: 420, Loss: 0.1383, Valid: 72.91%, Test: 47.27%\n",
      "learning rate: 0.0005645\n",
      ": Hits@20: Epoch: 421, Loss: 0.1374, Valid: 73.32%, Test: 86.85%\n",
      "learning rate: 0.0005628\n",
      ": Hits@20: Epoch: 422, Loss: 0.1377, Valid: 73.18%, Test: 93.97%\n",
      "learning rate: 0.0005612\n",
      ": Hits@20: Epoch: 423, Loss: 0.1396, Valid: 73.13%, Test: 69.99%\n",
      "learning rate: 0.0005595\n",
      ": Hits@20: Epoch: 424, Loss: 0.1376, Valid: 73.58%, Test: 88.22%\n",
      "learning rate: 0.0005578\n",
      ": Hits@20: Epoch: 425, Loss: 0.1389, Valid: 73.44%, Test: 87.63%\n",
      "learning rate: 0.0005561\n",
      ": Hits@20: Epoch: 426, Loss: 0.1383, Valid: 73.60%, Test: 85.46%\n",
      "learning rate: 0.0005545\n",
      ": Hits@20: Epoch: 427, Loss: 0.1387, Valid: 73.60%, Test: 73.39%\n",
      "learning rate: 0.0005528\n",
      ": Hits@20: Epoch: 428, Loss: 0.1377, Valid: 73.30%, Test: 89.30%\n",
      "learning rate: 0.0005511\n",
      ": Hits@20: Epoch: 429, Loss: 0.1383, Valid: 73.38%, Test: 70.27%\n",
      "learning rate: 0.0005495\n",
      ": Hits@20: Epoch: 430, Loss: 0.1382, Valid: 73.24%, Test: 62.25%\n",
      "learning rate: 0.0005478\n",
      ": Hits@20: Epoch: 431, Loss: 0.1373, Valid: 73.42%, Test: 84.12%\n",
      "learning rate: 0.0005462\n",
      ": Hits@20: Epoch: 432, Loss: 0.1377, Valid: 73.05%, Test: 91.97%\n",
      "learning rate: 0.0005445\n",
      ": Hits@20: Epoch: 433, Loss: 0.1370, Valid: 73.49%, Test: 82.54%\n",
      "learning rate: 0.0005429\n",
      ": Hits@20: Epoch: 434, Loss: 0.1386, Valid: 73.63%, Test: 82.90%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0005413\n",
      ": Hits@20: Epoch: 435, Loss: 0.1374, Valid: 73.00%, Test: 72.58%\n",
      "learning rate: 0.0005397\n",
      ": Hits@20: Epoch: 436, Loss: 0.1377, Valid: 73.43%, Test: 80.74%\n",
      "learning rate: 0.0005380\n",
      ": Hits@20: Epoch: 437, Loss: 0.1384, Valid: 73.32%, Test: 88.12%\n",
      "learning rate: 0.0005364\n",
      ": Hits@20: Epoch: 438, Loss: 0.1376, Valid: 73.32%, Test: 92.89%\n",
      "learning rate: 0.0005348\n",
      ": Hits@20: Epoch: 439, Loss: 0.1371, Valid: 73.51%, Test: 91.44%\n",
      "learning rate: 0.0005332\n",
      ": Hits@20: Epoch: 440, Loss: 0.1389, Valid: 73.56%, Test: 75.67%\n",
      "learning rate: 0.0005316\n",
      ": Hits@20: Epoch: 441, Loss: 0.1378, Valid: 72.95%, Test: 49.37%\n",
      "learning rate: 0.0005300\n",
      ": Hits@20: Epoch: 442, Loss: 0.1372, Valid: 73.80%, Test: 94.08%\n",
      "learning rate: 0.0005284\n",
      ": Hits@20: Epoch: 443, Loss: 0.1380, Valid: 73.54%, Test: 77.46%\n",
      "learning rate: 0.0005268\n",
      ": Hits@20: Epoch: 444, Loss: 0.1375, Valid: 73.57%, Test: 86.94%\n",
      "learning rate: 0.0005253\n",
      ": Hits@20: Epoch: 445, Loss: 0.1369, Valid: 73.38%, Test: 90.15%\n",
      "learning rate: 0.0005237\n",
      ": Hits@20: Epoch: 446, Loss: 0.1370, Valid: 73.31%, Test: 91.71%\n",
      "learning rate: 0.0005221\n",
      ": Hits@20: Epoch: 447, Loss: 0.1370, Valid: 73.11%, Test: 93.48%\n",
      "learning rate: 0.0005205\n",
      ": Hits@20: Epoch: 448, Loss: 0.1366, Valid: 73.64%, Test: 91.78%\n",
      "learning rate: 0.0005190\n",
      ": Hits@20: Epoch: 449, Loss: 0.1377, Valid: 73.47%, Test: 91.20%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0005174\n",
      ": Hits@20: Epoch: 450, Loss: 0.1366, Valid: 73.57%, Test: 88.01%\n",
      "learning rate: 0.0005159\n",
      ": Hits@20: Epoch: 451, Loss: 0.1369, Valid: 73.51%, Test: 84.92%\n",
      "learning rate: 0.0005143\n",
      ": Hits@20: Epoch: 452, Loss: 0.1382, Valid: 73.63%, Test: 82.95%\n",
      "learning rate: 0.0005128\n",
      ": Hits@20: Epoch: 453, Loss: 0.1365, Valid: 73.48%, Test: 81.25%\n",
      "learning rate: 0.0005112\n",
      ": Hits@20: Epoch: 454, Loss: 0.1384, Valid: 73.78%, Test: 92.00%\n",
      "learning rate: 0.0005097\n",
      ": Hits@20: Epoch: 455, Loss: 0.1372, Valid: 73.99%, Test: 91.56%\n",
      "learning rate: 0.0005082\n",
      ": Hits@20: Epoch: 456, Loss: 0.1386, Valid: 73.65%, Test: 89.43%\n",
      "learning rate: 0.0005067\n",
      ": Hits@20: Epoch: 457, Loss: 0.1368, Valid: 73.20%, Test: 83.22%\n",
      "learning rate: 0.0005051\n",
      ": Hits@20: Epoch: 458, Loss: 0.1376, Valid: 73.52%, Test: 90.70%\n",
      "learning rate: 0.0005036\n",
      ": Hits@20: Epoch: 459, Loss: 0.1369, Valid: 73.76%, Test: 92.79%\n",
      "learning rate: 0.0005021\n",
      ": Hits@20: Epoch: 460, Loss: 0.1373, Valid: 73.67%, Test: 91.85%\n",
      "learning rate: 0.0005006\n",
      ": Hits@20: Epoch: 461, Loss: 0.1380, Valid: 73.81%, Test: 91.11%\n",
      "learning rate: 0.0004991\n",
      ": Hits@20: Epoch: 462, Loss: 0.1369, Valid: 73.73%, Test: 93.16%\n",
      "learning rate: 0.0004976\n",
      ": Hits@20: Epoch: 463, Loss: 0.1371, Valid: 73.68%, Test: 93.85%\n",
      "learning rate: 0.0004961\n",
      ": Hits@20: Epoch: 464, Loss: 0.1364, Valid: 73.58%, Test: 67.71%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0004946\n",
      ": Hits@20: Epoch: 465, Loss: 0.1370, Valid: 73.73%, Test: 85.35%\n",
      "learning rate: 0.0004931\n",
      ": Hits@20: Epoch: 466, Loss: 0.1364, Valid: 73.84%, Test: 93.85%\n",
      "learning rate: 0.0004917\n",
      ": Hits@20: Epoch: 467, Loss: 0.1371, Valid: 73.85%, Test: 78.62%\n",
      "learning rate: 0.0004902\n",
      ": Hits@20: Epoch: 468, Loss: 0.1370, Valid: 73.88%, Test: 92.12%\n",
      "learning rate: 0.0004887\n",
      ": Hits@20: Epoch: 469, Loss: 0.1367, Valid: 73.91%, Test: 85.70%\n",
      "learning rate: 0.0004873\n",
      ": Hits@20: Epoch: 470, Loss: 0.1374, Valid: 73.92%, Test: 89.99%\n",
      "learning rate: 0.0004858\n",
      ": Hits@20: Epoch: 471, Loss: 0.1369, Valid: 73.61%, Test: 85.98%\n",
      "learning rate: 0.0004843\n",
      ": Hits@20: Epoch: 472, Loss: 0.1377, Valid: 73.73%, Test: 82.45%\n",
      "learning rate: 0.0004829\n",
      ": Hits@20: Epoch: 473, Loss: 0.1370, Valid: 73.82%, Test: 79.48%\n",
      "learning rate: 0.0004814\n",
      ": Hits@20: Epoch: 474, Loss: 0.1361, Valid: 73.95%, Test: 82.50%\n",
      "learning rate: 0.0004800\n",
      ": Hits@20: Epoch: 475, Loss: 0.1372, Valid: 73.93%, Test: 79.26%\n",
      "learning rate: 0.0004785\n",
      ": Hits@20: Epoch: 476, Loss: 0.1360, Valid: 73.91%, Test: 89.44%\n",
      "learning rate: 0.0004771\n",
      ": Hits@20: Epoch: 477, Loss: 0.1366, Valid: 73.82%, Test: 94.17%\n",
      "learning rate: 0.0004757\n",
      ": Hits@20: Epoch: 478, Loss: 0.1363, Valid: 73.93%, Test: 85.29%\n",
      "learning rate: 0.0004743\n",
      ": Hits@20: Epoch: 479, Loss: 0.1356, Valid: 73.95%, Test: 90.60%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0004728\n",
      ": Hits@20: Epoch: 480, Loss: 0.1360, Valid: 73.91%, Test: 54.16%\n",
      "learning rate: 0.0004714\n",
      ": Hits@20: Epoch: 481, Loss: 0.1354, Valid: 73.90%, Test: 66.04%\n",
      "learning rate: 0.0004700\n",
      ": Hits@20: Epoch: 482, Loss: 0.1368, Valid: 74.03%, Test: 77.36%\n",
      "learning rate: 0.0004686\n",
      ": Hits@20: Epoch: 483, Loss: 0.1369, Valid: 73.92%, Test: 86.97%\n",
      "learning rate: 0.0004672\n",
      ": Hits@20: Epoch: 484, Loss: 0.1366, Valid: 73.93%, Test: 76.35%\n",
      "learning rate: 0.0004658\n",
      ": Hits@20: Epoch: 485, Loss: 0.1371, Valid: 74.02%, Test: 68.38%\n",
      "learning rate: 0.0004644\n",
      ": Hits@20: Epoch: 486, Loss: 0.1361, Valid: 73.92%, Test: 93.69%\n",
      "learning rate: 0.0004630\n",
      ": Hits@20: Epoch: 487, Loss: 0.1366, Valid: 73.92%, Test: 92.58%\n",
      "learning rate: 0.0004616\n",
      ": Hits@20: Epoch: 488, Loss: 0.1360, Valid: 73.97%, Test: 90.11%\n",
      "learning rate: 0.0004602\n",
      ": Hits@20: Epoch: 489, Loss: 0.1373, Valid: 73.82%, Test: 93.16%\n",
      "learning rate: 0.0004588\n",
      ": Hits@20: Epoch: 490, Loss: 0.1373, Valid: 73.81%, Test: 83.99%\n",
      "learning rate: 0.0004575\n",
      ": Hits@20: Epoch: 491, Loss: 0.1360, Valid: 73.91%, Test: 92.34%\n",
      "learning rate: 0.0004561\n",
      ": Hits@20: Epoch: 492, Loss: 0.1370, Valid: 73.75%, Test: 84.32%\n",
      "learning rate: 0.0004547\n",
      ": Hits@20: Epoch: 493, Loss: 0.1366, Valid: 73.83%, Test: 68.18%\n",
      "learning rate: 0.0004534\n",
      ": Hits@20: Epoch: 494, Loss: 0.1352, Valid: 73.83%, Test: 81.35%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0004520\n",
      ": Hits@20: Epoch: 495, Loss: 0.1364, Valid: 73.83%, Test: 68.08%\n",
      "learning rate: 0.0004506\n",
      ": Hits@20: Epoch: 496, Loss: 0.1368, Valid: 73.84%, Test: 67.06%\n",
      "learning rate: 0.0004493\n",
      ": Hits@20: Epoch: 497, Loss: 0.1363, Valid: 73.74%, Test: 75.21%\n",
      "learning rate: 0.0004479\n",
      ": Hits@20: Epoch: 498, Loss: 0.1372, Valid: 73.71%, Test: 89.02%\n",
      "learning rate: 0.0004466\n",
      ": Hits@20: Epoch: 499, Loss: 0.1356, Valid: 73.72%, Test: 94.92%\n",
      "learning rate: 0.0004453\n",
      ": Hits@20: Epoch: 500, Loss: 0.1364, Valid: 73.77%, Test: 78.85%\n",
      "learning rate: 0.0004439\n",
      ": Hits@20: Epoch: 501, Loss: 0.1357, Valid: 73.82%, Test: 94.10%\n",
      "learning rate: 0.0004426\n",
      ": Hits@20: Epoch: 502, Loss: 0.1363, Valid: 73.69%, Test: 94.31%\n",
      "learning rate: 0.0004413\n",
      ": Hits@20: Epoch: 503, Loss: 0.1367, Valid: 73.66%, Test: 88.86%\n",
      "learning rate: 0.0004399\n",
      ": Hits@20: Epoch: 504, Loss: 0.1363, Valid: 73.87%, Test: 89.69%\n",
      "learning rate: 0.0004386\n",
      ": Hits@20: Epoch: 505, Loss: 0.1357, Valid: 73.79%, Test: 81.28%\n",
      "learning rate: 0.0004373\n",
      ": Hits@20: Epoch: 506, Loss: 0.1367, Valid: 73.95%, Test: 89.35%\n",
      "learning rate: 0.0004360\n",
      ": Hits@20: Epoch: 507, Loss: 0.1355, Valid: 73.67%, Test: 78.67%\n",
      "learning rate: 0.0004347\n",
      ": Hits@20: Epoch: 508, Loss: 0.1363, Valid: 73.76%, Test: 67.58%\n",
      "learning rate: 0.0004334\n",
      ": Hits@20: Epoch: 509, Loss: 0.1354, Valid: 73.81%, Test: 64.90%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0004321\n",
      ": Hits@20: Epoch: 510, Loss: 0.1366, Valid: 73.98%, Test: 84.87%\n",
      "learning rate: 0.0004308\n",
      ": Hits@20: Epoch: 511, Loss: 0.1353, Valid: 73.83%, Test: 91.71%\n",
      "learning rate: 0.0004295\n",
      ": Hits@20: Epoch: 512, Loss: 0.1356, Valid: 73.76%, Test: 94.70%\n",
      "learning rate: 0.0004282\n",
      ": Hits@20: Epoch: 513, Loss: 0.1354, Valid: 73.86%, Test: 83.16%\n",
      "learning rate: 0.0004269\n",
      ": Hits@20: Epoch: 514, Loss: 0.1353, Valid: 73.88%, Test: 92.02%\n",
      "learning rate: 0.0004256\n",
      ": Hits@20: Epoch: 515, Loss: 0.1356, Valid: 73.77%, Test: 92.53%\n",
      "learning rate: 0.0004244\n",
      ": Hits@20: Epoch: 516, Loss: 0.1364, Valid: 73.82%, Test: 80.20%\n",
      "learning rate: 0.0004231\n",
      ": Hits@20: Epoch: 517, Loss: 0.1362, Valid: 73.36%, Test: 86.64%\n",
      "learning rate: 0.0004218\n",
      ": Hits@20: Epoch: 518, Loss: 0.1356, Valid: 73.49%, Test: 80.70%\n",
      "learning rate: 0.0004205\n",
      ": Hits@20: Epoch: 519, Loss: 0.1373, Valid: 73.76%, Test: 81.99%\n",
      "learning rate: 0.0004193\n",
      ": Hits@20: Epoch: 520, Loss: 0.1348, Valid: 73.68%, Test: 91.86%\n",
      "learning rate: 0.0004180\n",
      ": Hits@20: Epoch: 521, Loss: 0.1355, Valid: 74.03%, Test: 85.89%\n",
      "learning rate: 0.0004168\n",
      ": Hits@20: Epoch: 522, Loss: 0.1351, Valid: 73.74%, Test: 79.74%\n",
      "learning rate: 0.0004155\n",
      ": Hits@20: Epoch: 523, Loss: 0.1358, Valid: 73.82%, Test: 90.49%\n",
      "learning rate: 0.0004143\n",
      ": Hits@20: Epoch: 524, Loss: 0.1358, Valid: 73.74%, Test: 83.37%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0004130\n",
      ": Hits@20: Epoch: 525, Loss: 0.1356, Valid: 73.46%, Test: 82.38%\n",
      "learning rate: 0.0004118\n",
      ": Hits@20: Epoch: 526, Loss: 0.1360, Valid: 73.30%, Test: 86.91%\n",
      "learning rate: 0.0004106\n",
      ": Hits@20: Epoch: 527, Loss: 0.1357, Valid: 73.33%, Test: 90.87%\n",
      "learning rate: 0.0004093\n",
      ": Hits@20: Epoch: 528, Loss: 0.1354, Valid: 73.61%, Test: 94.37%\n",
      "learning rate: 0.0004081\n",
      ": Hits@20: Epoch: 529, Loss: 0.1358, Valid: 73.68%, Test: 92.56%\n",
      "learning rate: 0.0004069\n",
      ": Hits@20: Epoch: 530, Loss: 0.1357, Valid: 73.82%, Test: 84.03%\n",
      "learning rate: 0.0004057\n",
      ": Hits@20: Epoch: 531, Loss: 0.1356, Valid: 73.97%, Test: 86.91%\n",
      "learning rate: 0.0004044\n",
      ": Hits@20: Epoch: 532, Loss: 0.1352, Valid: 73.70%, Test: 93.17%\n",
      "learning rate: 0.0004032\n",
      ": Hits@20: Epoch: 533, Loss: 0.1351, Valid: 73.69%, Test: 92.86%\n",
      "learning rate: 0.0004020\n",
      ": Hits@20: Epoch: 534, Loss: 0.1362, Valid: 73.36%, Test: 89.57%\n",
      "learning rate: 0.0004008\n",
      ": Hits@20: Epoch: 535, Loss: 0.1349, Valid: 73.69%, Test: 94.62%\n",
      "learning rate: 0.0003996\n",
      ": Hits@20: Epoch: 536, Loss: 0.1351, Valid: 73.78%, Test: 94.01%\n",
      "learning rate: 0.0003984\n",
      ": Hits@20: Epoch: 537, Loss: 0.1351, Valid: 74.03%, Test: 86.27%\n",
      "learning rate: 0.0003972\n",
      ": Hits@20: Epoch: 538, Loss: 0.1355, Valid: 74.09%, Test: 92.17%\n",
      "learning rate: 0.0003960\n",
      ": Hits@20: Epoch: 539, Loss: 0.1353, Valid: 73.87%, Test: 93.08%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0003948\n",
      ": Hits@20: Epoch: 540, Loss: 0.1350, Valid: 73.90%, Test: 91.98%\n",
      "learning rate: 0.0003937\n",
      ": Hits@20: Epoch: 541, Loss: 0.1353, Valid: 73.85%, Test: 92.76%\n",
      "learning rate: 0.0003925\n",
      ": Hits@20: Epoch: 542, Loss: 0.1361, Valid: 73.87%, Test: 74.72%\n",
      "learning rate: 0.0003913\n",
      ": Hits@20: Epoch: 543, Loss: 0.1360, Valid: 73.73%, Test: 43.39%\n",
      "learning rate: 0.0003901\n",
      ": Hits@20: Epoch: 544, Loss: 0.1349, Valid: 73.65%, Test: 90.88%\n",
      "learning rate: 0.0003889\n",
      ": Hits@20: Epoch: 545, Loss: 0.1349, Valid: 74.01%, Test: 93.96%\n",
      "learning rate: 0.0003878\n",
      ": Hits@20: Epoch: 546, Loss: 0.1351, Valid: 73.97%, Test: 91.38%\n",
      "learning rate: 0.0003866\n",
      ": Hits@20: Epoch: 547, Loss: 0.1347, Valid: 73.83%, Test: 55.18%\n",
      "learning rate: 0.0003855\n",
      ": Hits@20: Epoch: 548, Loss: 0.1355, Valid: 73.68%, Test: 80.80%\n",
      "learning rate: 0.0003843\n",
      ": Hits@20: Epoch: 549, Loss: 0.1350, Valid: 73.76%, Test: 93.95%\n",
      "learning rate: 0.0003831\n",
      ": Hits@20: Epoch: 550, Loss: 0.1349, Valid: 73.65%, Test: 77.63%\n",
      "learning rate: 0.0003820\n",
      ": Hits@20: Epoch: 551, Loss: 0.1349, Valid: 73.92%, Test: 92.79%\n",
      "learning rate: 0.0003809\n",
      ": Hits@20: Epoch: 552, Loss: 0.1360, Valid: 74.00%, Test: 82.49%\n",
      "learning rate: 0.0003797\n",
      ": Hits@20: Epoch: 553, Loss: 0.1346, Valid: 74.01%, Test: 95.30%\n",
      "learning rate: 0.0003786\n",
      ": Hits@20: Epoch: 554, Loss: 0.1353, Valid: 73.71%, Test: 93.93%\n",
      "negative sampling ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(args.epochs):\n",
    "    data_neg_train.resample_neg_edges()\n",
    "\n",
    "    loss = train(args, predictor, optimizer, scheduler, data_pos_train, data_neg_train)\n",
    "    lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "    if lr_now > args.lr_mini:\n",
    "        scheduler.step()\n",
    "\n",
    "    if epoch % args.eval_epoch == args.eval_epoch - 1:\n",
    "        # with Timing(name='Times of validation: '):\n",
    "        val_pred, val_true = test(args, predictor, data_pos_valid, data_neg_valid)\n",
    "        test_pred, test_true = test(args, predictor, data_pos_test, data_neg_test)\n",
    "        results = get_eval_result(args, val_pred, val_true, test_pred, test_true)\n",
    "        for key, result in results.items():\n",
    "            loggers[key].add_result(result)\n",
    "            valid_res, test_res = result\n",
    "            to_print = (f'learning rate: {lr_now:.7f}' + '\\n'\n",
    "                        + f': {key}: Epoch: {epoch:02d}, '\n",
    "                        + f'Loss: {loss:.4f}, Valid: {100 * valid_res:.2f}%, '\n",
    "                        + f'Test: {100 * test_res:.2f}%')\n",
    "            # tensorboard_writer.add_scalar('eval/loss', loss, epoch + 1)\n",
    "            # tensorboard_writer.add_scalar('eval/Valid', valid_res, epoch + 1)\n",
    "            # tensorboard_writer.add_scalar('eval/Test', test_res, epoch + 1)\n",
    "            with open(log_file, 'a') as f:\n",
    "                print(to_print, file=f)\n",
    "                print(to_print)\n",
    "\n",
    "\n",
    "for key in loggers.keys():\n",
    "    to_print = key\n",
    "    with open(log_file, 'a') as f:\n",
    "        print(to_print, file=f)\n",
    "        loggers[key].print_statistics(f=f)\n",
    "        print(to_print)\n",
    "        loggers[key].print_statistics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
