{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import *\n",
    "from models import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='ComHG for link prediction')\n",
    "parser.add_argument('--float', default=np.float32)\n",
    "parser.add_argument('--dataset', type=str, default='ogbl-ddi')  # ddi collab ppa citation2\n",
    "parser.add_argument('--result_appendix', type=str, default='', help=\"if '', time as appendix\")\n",
    "parser.add_argument('--device', type=str, default='0', help=\"cpu or gpu id\") \n",
    "parser.add_argument('--directed', type=bool, default=False)\n",
    "parser.add_argument('--coalesce', type=bool, default=True, help=\"whether to coalesce multiple edges between two nodes\")\n",
    "parser.add_argument('--use_weight', type=bool, default=False, help=\"whether to use edge weight.\")\n",
    "parser.add_argument('--use_val', type=bool, default=False, help=\"whether to use edges in validation set for testing\")\n",
    "parser.add_argument('--collab_year', type=int, default=2010, help=\"for ogbl-collab.\")\n",
    "\n",
    "parser.add_argument('--use_feature', type=bool, default=False)\n",
    "parser.add_argument('--use_node_emb', type=bool, default=True)\n",
    "parser.add_argument('--use_dist', type=bool, default=False, help=\"whether to use shortest path distance\")\n",
    "parser.add_argument('--use_cn', type=bool, default=False, help=\"whether to use common neighbor\")\n",
    "parser.add_argument('--use_aa', type=bool, default=False, help=\"whether to use Adamic/Adar\")\n",
    "parser.add_argument('--use_ja', type=bool, default=False, help=\"whether to use Jaccard\")\n",
    "parser.add_argument('--use_ra', type=bool, default=False, help=\"whether to use Resource Allocation\")\n",
    "parser.add_argument('--use_degree', type=bool, default=False)\n",
    "\n",
    "parser.add_argument('--max_dist', type=int, default=5)\n",
    "parser.add_argument('--max_cn', type=int, default=100)\n",
    "parser.add_argument('--max_aa', type=int, default=100)\n",
    "parser.add_argument('--max_ja', type=int, default=100)\n",
    "parser.add_argument('--max_ra', type=int, default=20)\n",
    "parser.add_argument('--max_degree', type=int, default=1000)\n",
    "parser.add_argument('--mag_ja', type=int, default=100)\n",
    "parser.add_argument('--mag_aa', type=int, default=10)\n",
    "parser.add_argument('--mag_ra', type=int, default=10)\n",
    "\n",
    "parser.add_argument('--heurisctic_reproduce', type=bool, default=False, help=\"whether to re-produce heuristics, False would speed up training but may decrease the performance\")\n",
    "parser.add_argument('--heurisctic_batch_size', type=int, default=100, help=\"number of rows in distance computation\")\n",
    "parser.add_argument('--heurisctic_directed', type=bool, default=False, help=\"use directed graph in distance computation\")\n",
    "parser.add_argument('--heurisctic_reuse', type=bool, default=True)\n",
    "parser.add_argument('--neg_size', type=float, default=2000, help=\"size of data_neg_train: neg_size * len(split_edge['train']['edge'])\")\n",
    "\n",
    "parser.add_argument('--adj_hop', type=int, default=2, help=\"hieghest hop of adj for GNN.\")\n",
    "parser.add_argument('--adj_neg', type=float, default=0.0, help=\"neg samples (global neighbors) used in adj. \")\n",
    "parser.add_argument('--adj_neg_dist', type=int, default=10, help=\"distance from global neighbors to the central node.\")\n",
    "parser.add_argument('--adj_weight', type=str, default='same', help=\"the method for weights in the original adj. options: same or decay\")\n",
    "\n",
    "parser.add_argument('--atten_type', type=str, default='Multiply',\n",
    "                    help=\"attention mechnism. Options: Concat, Cosine, Multiply, no_atten, with repsect to GAT, AGNN, Transformer and GCN\")\n",
    "parser.add_argument('--atten_combine', type=str, default='plus',\n",
    "                    help=\"type of combining original adj with attention (if use). must be: plus, multiply or only_atten\")\n",
    "parser.add_argument('--bias', type=bool, default=True)\n",
    "parser.add_argument('--dim_node_emb', type=int, default=512)\n",
    "parser.add_argument('--dim_encoding', type=int, default=32, help=\"dim for encoding heuristics, etc.\")\n",
    "parser.add_argument('--dim_atten', type=int, default=8, help=\"dim for matrix multiplication. Should be small for large graph\")\n",
    "parser.add_argument('--dim_hidden', type=int, default=None, help=\"if None, dim_hidden = dim_in\")\n",
    "parser.add_argument('--n_layers', type=int, default=2, help=\"CNN layers\")\n",
    "parser.add_argument('--n_heads', type=int, default=4, help=\"multiply head CNN\")\n",
    "parser.add_argument('--n_layers_mlp', type=int, default=5)\n",
    "parser.add_argument('--residual', type=bool, default=True, help=\"whether to use residual connection in CNN\")\n",
    "parser.add_argument('--reduce', type=str, default='add', help=\"combine outputs of multi-head CNN modules. options: concat, add\")\n",
    "parser.add_argument('--negative_slope', type=float, default=0.2, help=\"negative_slope for leaky_relu\")\n",
    "\n",
    "parser.add_argument('--num_workers', type=int, default=64)\n",
    "parser.add_argument('--optimizer', type=str, default='Adam', help=\"'Adam', 'AdamW', 'SGD'\")\n",
    "parser.add_argument('--clip_grad_norm', type=float, default=1.0, help=\"whether to use clip_grad_norm_ in training\")\n",
    "parser.add_argument('--use_layer_norm', type=bool, default=False, help=\"whether to use layer norm\")\n",
    "parser.add_argument('--dropout', type=float, default=0.25)\n",
    "parser.add_argument('--dropout_adj', type=float, default=0.)\n",
    "parser.add_argument('--lr', type=float, default=0.002)\n",
    "parser.add_argument('--lr_mini', type=float, default=0.00001, help=\"lr stops decreasing at lr_mini\")\n",
    "parser.add_argument('--scheduler_gamma', type=float, default=0.997)\n",
    "parser.add_argument('--shuffle', type=bool, default=True)\n",
    "\n",
    "parser.add_argument('--runs', type=int, default=1)\n",
    "parser.add_argument('--epochs', type=int, default=1000)\n",
    "parser.add_argument('--eval_epoch', type=int, default=1)\n",
    "parser.add_argument('--batch_size', type=int, default=100000)\n",
    "parser.add_argument('--batch_num', type=int, default=1000, help=\"number of batches trained in an epoch\")\n",
    "\n",
    "parser = add_hierarchical_args(parser)\n",
    "args = parser.parse_args([])  # Empty list since we're in a notebook\n",
    "args.device = device\n",
    "args.max_dist = max(args.max_dist, 3) if args.use_dist else 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved in results/ogbl-ddi_20241212172435\n",
      "Namespace(float=<class 'numpy.float32'>, dataset='ogbl-ddi', result_appendix='_20241212172435', device=device(type='cuda'), directed=False, coalesce=True, use_weight=False, use_val=False, collab_year=2010, use_feature=False, use_node_emb=True, use_dist=False, use_cn=False, use_aa=False, use_ja=False, use_ra=False, use_degree=False, max_dist=3, max_cn=100, max_aa=100, max_ja=100, max_ra=20, max_degree=1000, mag_ja=100, mag_aa=10, mag_ra=10, heurisctic_reproduce=False, heurisctic_batch_size=100, heurisctic_directed=False, heurisctic_reuse=True, neg_size=2000, adj_hop=2, adj_neg=0.0, adj_neg_dist=10, adj_weight='same', atten_type='Multiply', atten_combine='plus', bias=True, dim_node_emb=512, dim_encoding=32, dim_atten=8, dim_hidden=None, n_layers=2, n_heads=4, n_layers_mlp=5, residual=True, reduce='add', negative_slope=0.2, num_workers=64, optimizer='Adam', clip_grad_norm=1.0, use_layer_norm=False, dropout=0.25, dropout_adj=0.0, lr=0.002, lr_mini=1e-05, scheduler_gamma=0.997, shuffle=True, runs=1, epochs=1000, eval_epoch=1, batch_size=100000, batch_num=1000, text_dim=768, relation_dim=100, num_communities=100, eval_metrics='Hits@20', hitK=[20], dense_sparse='dense', dir_result='results/ogbl-ddi_20241212172435')\n"
     ]
    }
   ],
   "source": [
    "# Dataset loading and preprocessing\n",
    "dataset = PygLinkPropPredDataset(name=args.dataset)\n",
    "split_edge = dataset.get_edge_split()\n",
    "data = dataset[0]\n",
    "\n",
    "args.eval_metrics = 'Hits@20'\n",
    "args.hitK = [20]\n",
    "args.dense_sparse = 'dense'\n",
    "\n",
    "# Set up results directory\n",
    "if args.result_appendix == '':\n",
    "    args.result_appendix = '_' + time.strftime(\"%Y%m%d%H%M%S\")\n",
    "args.dir_result = os.path.join('results/{}{}'.format(args.dataset, args.result_appendix))\n",
    "print('Results will be saved in ' + args.dir_result)\n",
    "\n",
    "if not os.path.exists(args.dir_result):\n",
    "    os.makedirs(args.dir_result)\n",
    "\n",
    "# Copy source files to results directory\n",
    "for file in ['main_pred.py', 'utils.py', 'models.py']:\n",
    "    copy(file, args.dir_result)\n",
    "\n",
    "# Set up logging\n",
    "loggers = get_loggers(args)\n",
    "log_file = osp.join(args.dir_result, 'log.log')\n",
    "\n",
    "# Log arguments\n",
    "with open(log_file, 'w') as f:\n",
    "    print(str(args), file=f)\n",
    "    print(str(args), file=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove non-shortest distance neighbors in high adjs ...\n",
      "saving adj_degree data to dataset/ogbl_ddi/processed/adj_hop2_neg0.0.pt\n",
      "finish loading data_pos_train\n",
      "finish loading data_pos_valid\n",
      "finish loading data_neg_valid\n",
      "finish loading data_pos_test\n",
      "finish loading data_neg_test\n"
     ]
    }
   ],
   "source": [
    "data_pos_train = graph_prepare(args, posneg_split='pos_train')\n",
    "data_pos_valid = graph_prepare(args, posneg_split='pos_valid')\n",
    "data_neg_valid = graph_prepare(args, posneg_split='neg_valid')\n",
    "data_pos_test = graph_prepare(args, posneg_split='pos_test')\n",
    "data_neg_test = graph_prepare(args, posneg_split='neg_test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dim_in(args, data):\n",
    "    if data.x != None:\n",
    "        x_size = data.x.size(-1)\n",
    "        if args.dataset == 'ogbl-ppa':\n",
    "            x_size = args.dim_encoding\n",
    "    args.dim_in = 0\n",
    "    if args.use_feature and data.x != None:\n",
    "        args.dim_in += x_size\n",
    "    if args.use_node_emb:\n",
    "        args.dim_in += args.dim_node_emb\n",
    "    if args.use_degree:\n",
    "        args.dim_in += args.dim_encoding\n",
    "    if args.dim_in == 0:\n",
    "        args.dim_in = 1\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_DataLoader(args, data, shuffle=False):\n",
    "    edges = data.edges\n",
    "    num_edge = edges.size(0)\n",
    "    perm_size = int(min(args.batch_num * args.batch_size, num_edge))\n",
    "    if shuffle:\n",
    "        if num_edge > 1E8:\n",
    "            perm = np.array(random.sample(range(num_edge), perm_size))\n",
    "        else:\n",
    "            perm = np.random.permutation(num_edge)\n",
    "            perm = perm[:perm_size]\n",
    "        edges = edges[perm]\n",
    "    step, end = args.batch_size, perm_size\n",
    "    perms = [np.array(range(i, i + step)) if i + step < end else np.array(range(i, end)) for i in range(0, end, step)]\n",
    "\n",
    "    return edges, perms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, predictor, optimizer, scheduler, data_pos, data_neg):\n",
    "    #\n",
    "    predictor.train()\n",
    "    running_loss = running_examples = 0\n",
    "\n",
    "    pos_edges, pos_perms = my_DataLoader(args, data_pos, shuffle=args.shuffle)\n",
    "    neg_edges, neg_perms = my_DataLoader(args, data_neg, shuffle=args.shuffle)\n",
    "    leniter = min(len(pos_perms), len(neg_perms))\n",
    "    for i in range(leniter):\n",
    "        optimizer.zero_grad()\n",
    "        edge_pos = pos_edges[pos_perms[i]]\n",
    "        edge_neg = neg_edges[neg_perms[i]]\n",
    "        y_pos = predictor(data_pos, edge_pos)\n",
    "        y_neg = predictor(data_neg, edge_neg)\n",
    "        # avoid imbalance\n",
    "        if len(y_pos) != len(y_neg):\n",
    "            break\n",
    "\n",
    "        pos_loss = (-torch.log(y_pos + 1e-15)).mean()\n",
    "        neg_loss = (-torch.log(1.0 - y_neg + 1e-15)).mean()\n",
    "        loss = pos_loss + neg_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(predictor.parameters(), args.clip_grad_norm)#, error_if_nonfinite=True\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_examples += 1\n",
    "    return running_loss / running_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(args, predictor, data_pos, data_neg):\n",
    "    predictor.eval()\n",
    "\n",
    "    def get_predictions(args, predictor, data_pred):\n",
    "        pred = []\n",
    "        edges = data_pred.edges\n",
    "        num_edge = edges.size(0)\n",
    "        step, end = args.batch_size*100, num_edge\n",
    "        perms = [np.array(range(i, i + step)) if i + step < end else np.array(range(i, end)) for i in range(0, end, step)]\n",
    "        for perm in perms:\n",
    "            edge_batch = edges[perm]\n",
    "            y = predictor(data_pred, edge_batch)\n",
    "            pred.append(y.detach().cpu().squeeze(1))\n",
    "        return torch.cat(pred, dim=0)\n",
    "\n",
    "    pred = torch.cat([get_predictions(args, predictor,data_pos), get_predictions(args, predictor,data_neg)], dim=0)\n",
    "    true = torch.cat([torch.ones(data_pos.edges.size(0)), torch.zeros(data_neg.edges.size(0))], dim=0)\n",
    "\n",
    "    return pred, true\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative sampling ...\n",
      "negative sampling finished\n",
      "finish loading data_neg_train\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# tensorboard_writer = SummaryWriter(osp.join(args.dir_result, f'log_{run}.log'))\u001b[39;00m\n\u001b[1;32m      4\u001b[0m args \u001b[38;5;241m=\u001b[39m get_dim_in(args, data_pos_train)\n\u001b[0;32m----> 5\u001b[0m predictor \u001b[38;5;241m=\u001b[39m HierarchicalComHG(args)\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_parameters:\u001b[39m\u001b[38;5;124m'\u001b[39m, count_parameters(predictor))\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m get_optimizer(args, predictor\u001b[38;5;241m.\u001b[39mparameters())\n",
      "File \u001b[0;32m/n/scratch/users/r/rh236/GCN/models.py:285\u001b[0m, in \u001b[0;36mHierarchicalComHG.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommunity_encoder \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(args\u001b[38;5;241m.\u001b[39mnum_communities, args\u001b[38;5;241m.\u001b[39mdim_encoding)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# Additional MLP for combining hierarchical features\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhierarchical_mlp \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m--> 285\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(args\u001b[38;5;241m.\u001b[39mdim_hidden \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m, args\u001b[38;5;241m.\u001b[39mdim_hidden \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m    286\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m    287\u001b[0m     nn\u001b[38;5;241m.\u001b[39mDropout(args\u001b[38;5;241m.\u001b[39mdropout),\n\u001b[1;32m    288\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(args\u001b[38;5;241m.\u001b[39mdim_hidden \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, args\u001b[38;5;241m.\u001b[39mdim_hidden)\n\u001b[1;32m    289\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "data_neg_train = graph_prepare(args, posneg_split='neg_train')\n",
    "\n",
    "# tensorboard_writer = SummaryWriter(osp.join(args.dir_result, f'log_{run}.log'))\n",
    "args = get_dim_in(args, data_pos_train)\n",
    "predictor = HierarchicalComHG(args).to(args.device)\n",
    "print('count_parameters:', count_parameters(predictor))\n",
    "optimizer = get_optimizer(args, predictor.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=args.scheduler_gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 0.0020000\n",
      ": Hits@20: Epoch: 00, Loss: 1.3364, Valid: 1.53%, Test: 2.26%\n",
      "learning rate: 0.0019940\n",
      ": Hits@20: Epoch: 01, Loss: 0.9610, Valid: 2.07%, Test: 4.77%\n",
      "learning rate: 0.0019880\n",
      ": Hits@20: Epoch: 02, Loss: 0.8770, Valid: 2.38%, Test: 4.34%\n",
      "learning rate: 0.0019821\n",
      ": Hits@20: Epoch: 03, Loss: 0.7888, Valid: 2.26%, Test: 2.84%\n",
      "learning rate: 0.0019761\n",
      ": Hits@20: Epoch: 04, Loss: 0.7648, Valid: 2.38%, Test: 2.92%\n",
      "learning rate: 0.0019702\n",
      ": Hits@20: Epoch: 05, Loss: 0.7039, Valid: 11.74%, Test: 6.87%\n",
      "learning rate: 0.0019643\n",
      ": Hits@20: Epoch: 06, Loss: 0.6466, Valid: 11.34%, Test: 7.86%\n",
      "learning rate: 0.0019584\n",
      ": Hits@20: Epoch: 07, Loss: 0.6141, Valid: 10.43%, Test: 9.35%\n",
      "learning rate: 0.0019525\n",
      ": Hits@20: Epoch: 08, Loss: 0.5860, Valid: 11.05%, Test: 15.10%\n",
      "learning rate: 0.0019466\n",
      ": Hits@20: Epoch: 09, Loss: 0.5745, Valid: 13.65%, Test: 9.91%\n",
      "learning rate: 0.0019408\n",
      ": Hits@20: Epoch: 10, Loss: 0.5348, Valid: 16.53%, Test: 9.04%\n",
      "learning rate: 0.0019350\n",
      ": Hits@20: Epoch: 11, Loss: 0.5045, Valid: 18.92%, Test: 9.12%\n",
      "learning rate: 0.0019292\n",
      ": Hits@20: Epoch: 12, Loss: 0.4877, Valid: 15.00%, Test: 8.40%\n",
      "learning rate: 0.0019234\n",
      ": Hits@20: Epoch: 13, Loss: 0.4962, Valid: 12.54%, Test: 2.47%\n",
      "learning rate: 0.0019176\n",
      ": Hits@20: Epoch: 14, Loss: 0.4644, Valid: 20.32%, Test: 7.67%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0019119\n",
      ": Hits@20: Epoch: 15, Loss: 0.4436, Valid: 18.23%, Test: 5.37%\n",
      "learning rate: 0.0019061\n",
      ": Hits@20: Epoch: 16, Loss: 0.4272, Valid: 20.01%, Test: 11.31%\n",
      "learning rate: 0.0019004\n",
      ": Hits@20: Epoch: 17, Loss: 0.4133, Valid: 14.43%, Test: 5.24%\n",
      "learning rate: 0.0018947\n",
      ": Hits@20: Epoch: 18, Loss: 0.3999, Valid: 15.20%, Test: 4.89%\n",
      "learning rate: 0.0018890\n",
      ": Hits@20: Epoch: 19, Loss: 0.4038, Valid: 18.33%, Test: 5.20%\n",
      "learning rate: 0.0018834\n",
      ": Hits@20: Epoch: 20, Loss: 0.3853, Valid: 12.53%, Test: 5.82%\n",
      "learning rate: 0.0018777\n",
      ": Hits@20: Epoch: 21, Loss: 0.3684, Valid: 20.78%, Test: 8.18%\n",
      "learning rate: 0.0018721\n",
      ": Hits@20: Epoch: 22, Loss: 0.3584, Valid: 13.46%, Test: 6.33%\n",
      "learning rate: 0.0018665\n",
      ": Hits@20: Epoch: 23, Loss: 0.3501, Valid: 23.27%, Test: 7.49%\n",
      "learning rate: 0.0018609\n",
      ": Hits@20: Epoch: 24, Loss: 0.3402, Valid: 21.23%, Test: 6.91%\n",
      "learning rate: 0.0018553\n",
      ": Hits@20: Epoch: 25, Loss: 0.3376, Valid: 23.28%, Test: 9.86%\n",
      "learning rate: 0.0018497\n",
      ": Hits@20: Epoch: 26, Loss: 0.3292, Valid: 18.70%, Test: 13.81%\n",
      "learning rate: 0.0018442\n",
      ": Hits@20: Epoch: 27, Loss: 0.3240, Valid: 20.79%, Test: 10.66%\n",
      "learning rate: 0.0018386\n",
      ": Hits@20: Epoch: 28, Loss: 0.3136, Valid: 20.05%, Test: 10.20%\n",
      "learning rate: 0.0018331\n",
      ": Hits@20: Epoch: 29, Loss: 0.3091, Valid: 22.68%, Test: 8.06%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0018276\n",
      ": Hits@20: Epoch: 30, Loss: 0.3012, Valid: 20.57%, Test: 12.79%\n",
      "learning rate: 0.0018221\n",
      ": Hits@20: Epoch: 31, Loss: 0.2950, Valid: 26.88%, Test: 14.56%\n",
      "learning rate: 0.0018167\n",
      ": Hits@20: Epoch: 32, Loss: 0.2912, Valid: 22.90%, Test: 14.40%\n",
      "learning rate: 0.0018112\n",
      ": Hits@20: Epoch: 33, Loss: 0.2906, Valid: 21.35%, Test: 12.79%\n",
      "learning rate: 0.0018058\n",
      ": Hits@20: Epoch: 34, Loss: 0.2852, Valid: 25.76%, Test: 11.75%\n",
      "learning rate: 0.0018004\n",
      ": Hits@20: Epoch: 35, Loss: 0.2784, Valid: 21.55%, Test: 17.89%\n",
      "learning rate: 0.0017950\n",
      ": Hits@20: Epoch: 36, Loss: 0.2761, Valid: 29.40%, Test: 17.24%\n",
      "learning rate: 0.0017896\n",
      ": Hits@20: Epoch: 37, Loss: 0.2733, Valid: 29.45%, Test: 19.20%\n",
      "learning rate: 0.0017842\n",
      ": Hits@20: Epoch: 38, Loss: 0.2675, Valid: 29.62%, Test: 19.05%\n",
      "learning rate: 0.0017789\n",
      ": Hits@20: Epoch: 39, Loss: 0.2671, Valid: 27.00%, Test: 14.11%\n",
      "learning rate: 0.0017735\n",
      ": Hits@20: Epoch: 40, Loss: 0.2632, Valid: 26.71%, Test: 19.88%\n",
      "learning rate: 0.0017682\n",
      ": Hits@20: Epoch: 41, Loss: 0.2570, Valid: 27.01%, Test: 24.73%\n",
      "learning rate: 0.0017629\n",
      ": Hits@20: Epoch: 42, Loss: 0.2528, Valid: 17.92%, Test: 15.86%\n",
      "learning rate: 0.0017576\n",
      ": Hits@20: Epoch: 43, Loss: 0.2494, Valid: 25.58%, Test: 18.77%\n",
      "learning rate: 0.0017523\n",
      ": Hits@20: Epoch: 44, Loss: 0.2469, Valid: 21.12%, Test: 27.52%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0017471\n",
      ": Hits@20: Epoch: 45, Loss: 0.2437, Valid: 23.66%, Test: 26.01%\n",
      "learning rate: 0.0017418\n",
      ": Hits@20: Epoch: 46, Loss: 0.2410, Valid: 29.74%, Test: 16.93%\n",
      "learning rate: 0.0017366\n",
      ": Hits@20: Epoch: 47, Loss: 0.2393, Valid: 32.33%, Test: 24.09%\n",
      "learning rate: 0.0017314\n",
      ": Hits@20: Epoch: 48, Loss: 0.2361, Valid: 34.33%, Test: 18.16%\n",
      "learning rate: 0.0017262\n",
      ": Hits@20: Epoch: 49, Loss: 0.2353, Valid: 26.00%, Test: 8.33%\n",
      "learning rate: 0.0017210\n",
      ": Hits@20: Epoch: 50, Loss: 0.2377, Valid: 33.98%, Test: 21.74%\n",
      "learning rate: 0.0017159\n",
      ": Hits@20: Epoch: 51, Loss: 0.2317, Valid: 24.23%, Test: 17.20%\n",
      "learning rate: 0.0017107\n",
      ": Hits@20: Epoch: 52, Loss: 0.2271, Valid: 35.63%, Test: 20.66%\n",
      "learning rate: 0.0017056\n",
      ": Hits@20: Epoch: 53, Loss: 0.2255, Valid: 36.55%, Test: 25.33%\n",
      "learning rate: 0.0017005\n",
      ": Hits@20: Epoch: 54, Loss: 0.2229, Valid: 39.54%, Test: 20.63%\n",
      "learning rate: 0.0016954\n",
      ": Hits@20: Epoch: 55, Loss: 0.2201, Valid: 35.50%, Test: 18.84%\n",
      "learning rate: 0.0016903\n",
      ": Hits@20: Epoch: 56, Loss: 0.2158, Valid: 38.95%, Test: 20.31%\n",
      "learning rate: 0.0016852\n",
      ": Hits@20: Epoch: 57, Loss: 0.2137, Valid: 43.27%, Test: 20.53%\n",
      "learning rate: 0.0016802\n",
      ": Hits@20: Epoch: 58, Loss: 0.2126, Valid: 43.69%, Test: 22.25%\n",
      "learning rate: 0.0016751\n",
      ": Hits@20: Epoch: 59, Loss: 0.2116, Valid: 40.08%, Test: 19.55%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0016701\n",
      ": Hits@20: Epoch: 60, Loss: 0.2097, Valid: 38.85%, Test: 16.25%\n",
      "learning rate: 0.0016651\n",
      ": Hits@20: Epoch: 61, Loss: 0.2093, Valid: 41.05%, Test: 13.28%\n",
      "learning rate: 0.0016601\n",
      ": Hits@20: Epoch: 62, Loss: 0.2054, Valid: 54.11%, Test: 16.70%\n",
      "learning rate: 0.0016551\n",
      ": Hits@20: Epoch: 63, Loss: 0.2050, Valid: 50.78%, Test: 16.69%\n",
      "learning rate: 0.0016501\n",
      ": Hits@20: Epoch: 64, Loss: 0.2037, Valid: 44.40%, Test: 14.29%\n",
      "learning rate: 0.0016452\n",
      ": Hits@20: Epoch: 65, Loss: 0.2018, Valid: 48.32%, Test: 13.44%\n",
      "learning rate: 0.0016403\n",
      ": Hits@20: Epoch: 66, Loss: 0.1997, Valid: 52.00%, Test: 15.77%\n",
      "learning rate: 0.0016353\n",
      ": Hits@20: Epoch: 67, Loss: 0.1970, Valid: 51.21%, Test: 13.52%\n",
      "learning rate: 0.0016304\n",
      ": Hits@20: Epoch: 68, Loss: 0.1963, Valid: 49.68%, Test: 13.76%\n",
      "learning rate: 0.0016255\n",
      ": Hits@20: Epoch: 69, Loss: 0.1948, Valid: 51.39%, Test: 15.69%\n",
      "learning rate: 0.0016207\n",
      ": Hits@20: Epoch: 70, Loss: 0.1944, Valid: 48.77%, Test: 14.31%\n",
      "learning rate: 0.0016158\n",
      ": Hits@20: Epoch: 71, Loss: 0.1928, Valid: 51.23%, Test: 12.40%\n",
      "learning rate: 0.0016109\n",
      ": Hits@20: Epoch: 72, Loss: 0.1919, Valid: 50.76%, Test: 13.11%\n",
      "learning rate: 0.0016061\n",
      ": Hits@20: Epoch: 73, Loss: 0.1908, Valid: 56.89%, Test: 12.96%\n",
      "learning rate: 0.0016013\n",
      ": Hits@20: Epoch: 74, Loss: 0.1887, Valid: 56.93%, Test: 10.40%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0015965\n",
      ": Hits@20: Epoch: 75, Loss: 0.1884, Valid: 55.38%, Test: 10.52%\n",
      "learning rate: 0.0015917\n",
      ": Hits@20: Epoch: 76, Loss: 0.1873, Valid: 57.28%, Test: 9.53%\n",
      "learning rate: 0.0015869\n",
      ": Hits@20: Epoch: 77, Loss: 0.1876, Valid: 57.20%, Test: 10.28%\n",
      "learning rate: 0.0015822\n",
      ": Hits@20: Epoch: 78, Loss: 0.1882, Valid: 54.85%, Test: 9.87%\n",
      "learning rate: 0.0015774\n",
      ": Hits@20: Epoch: 79, Loss: 0.1845, Valid: 49.04%, Test: 8.81%\n",
      "learning rate: 0.0015727\n",
      ": Hits@20: Epoch: 80, Loss: 0.1848, Valid: 54.44%, Test: 8.11%\n",
      "learning rate: 0.0015680\n",
      ": Hits@20: Epoch: 81, Loss: 0.1830, Valid: 58.22%, Test: 8.83%\n",
      "learning rate: 0.0015633\n",
      ": Hits@20: Epoch: 82, Loss: 0.1825, Valid: 54.84%, Test: 8.49%\n",
      "learning rate: 0.0015586\n",
      ": Hits@20: Epoch: 83, Loss: 0.1798, Valid: 59.76%, Test: 5.77%\n",
      "learning rate: 0.0015539\n",
      ": Hits@20: Epoch: 84, Loss: 0.1802, Valid: 60.26%, Test: 7.80%\n",
      "learning rate: 0.0015492\n",
      ": Hits@20: Epoch: 85, Loss: 0.1787, Valid: 60.29%, Test: 6.77%\n",
      "learning rate: 0.0015446\n",
      ": Hits@20: Epoch: 86, Loss: 0.1783, Valid: 61.44%, Test: 8.37%\n",
      "learning rate: 0.0015400\n",
      ": Hits@20: Epoch: 87, Loss: 0.1771, Valid: 58.99%, Test: 6.36%\n",
      "learning rate: 0.0015353\n",
      ": Hits@20: Epoch: 88, Loss: 0.1771, Valid: 60.89%, Test: 5.49%\n",
      "learning rate: 0.0015307\n",
      ": Hits@20: Epoch: 89, Loss: 0.1766, Valid: 59.25%, Test: 5.31%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0015261\n",
      ": Hits@20: Epoch: 90, Loss: 0.1759, Valid: 61.34%, Test: 6.51%\n",
      "learning rate: 0.0015216\n",
      ": Hits@20: Epoch: 91, Loss: 0.1761, Valid: 61.06%, Test: 10.83%\n",
      "learning rate: 0.0015170\n",
      ": Hits@20: Epoch: 92, Loss: 0.1733, Valid: 61.64%, Test: 8.98%\n",
      "learning rate: 0.0015124\n",
      ": Hits@20: Epoch: 93, Loss: 0.1734, Valid: 58.70%, Test: 8.02%\n",
      "learning rate: 0.0015079\n",
      ": Hits@20: Epoch: 94, Loss: 0.1729, Valid: 60.65%, Test: 6.67%\n",
      "learning rate: 0.0015034\n",
      ": Hits@20: Epoch: 95, Loss: 0.1716, Valid: 57.28%, Test: 7.45%\n",
      "learning rate: 0.0014989\n",
      ": Hits@20: Epoch: 96, Loss: 0.1714, Valid: 61.26%, Test: 5.79%\n",
      "learning rate: 0.0014944\n",
      ": Hits@20: Epoch: 97, Loss: 0.1710, Valid: 60.00%, Test: 3.88%\n",
      "learning rate: 0.0014899\n",
      ": Hits@20: Epoch: 98, Loss: 0.1701, Valid: 64.66%, Test: 8.57%\n",
      "learning rate: 0.0014854\n",
      ": Hits@20: Epoch: 99, Loss: 0.1710, Valid: 63.14%, Test: 8.12%\n",
      "learning rate: 0.0014810\n",
      ": Hits@20: Epoch: 100, Loss: 0.1688, Valid: 63.08%, Test: 11.21%\n",
      "learning rate: 0.0014765\n",
      ": Hits@20: Epoch: 101, Loss: 0.1686, Valid: 64.34%, Test: 10.39%\n",
      "learning rate: 0.0014721\n",
      ": Hits@20: Epoch: 102, Loss: 0.1678, Valid: 63.62%, Test: 6.29%\n",
      "learning rate: 0.0014677\n",
      ": Hits@20: Epoch: 103, Loss: 0.1682, Valid: 64.88%, Test: 15.25%\n",
      "learning rate: 0.0014633\n",
      ": Hits@20: Epoch: 104, Loss: 0.1665, Valid: 62.25%, Test: 8.56%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0014589\n",
      ": Hits@20: Epoch: 105, Loss: 0.1659, Valid: 62.77%, Test: 5.92%\n",
      "learning rate: 0.0014545\n",
      ": Hits@20: Epoch: 106, Loss: 0.1662, Valid: 64.34%, Test: 6.72%\n",
      "learning rate: 0.0014501\n",
      ": Hits@20: Epoch: 107, Loss: 0.1657, Valid: 61.94%, Test: 6.72%\n",
      "learning rate: 0.0014458\n",
      ": Hits@20: Epoch: 108, Loss: 0.1659, Valid: 62.41%, Test: 5.46%\n",
      "learning rate: 0.0014415\n",
      ": Hits@20: Epoch: 109, Loss: 0.1656, Valid: 62.66%, Test: 15.92%\n",
      "learning rate: 0.0014371\n",
      ": Hits@20: Epoch: 110, Loss: 0.1637, Valid: 62.22%, Test: 9.10%\n",
      "learning rate: 0.0014328\n",
      ": Hits@20: Epoch: 111, Loss: 0.1642, Valid: 62.44%, Test: 13.15%\n",
      "learning rate: 0.0014285\n",
      ": Hits@20: Epoch: 112, Loss: 0.1630, Valid: 62.28%, Test: 5.58%\n",
      "learning rate: 0.0014242\n",
      ": Hits@20: Epoch: 113, Loss: 0.1638, Valid: 64.16%, Test: 5.68%\n",
      "learning rate: 0.0014200\n",
      ": Hits@20: Epoch: 114, Loss: 0.1616, Valid: 65.68%, Test: 15.10%\n",
      "learning rate: 0.0014157\n",
      ": Hits@20: Epoch: 115, Loss: 0.1622, Valid: 64.11%, Test: 25.31%\n",
      "learning rate: 0.0014115\n",
      ": Hits@20: Epoch: 116, Loss: 0.1631, Valid: 62.36%, Test: 10.50%\n",
      "learning rate: 0.0014072\n",
      ": Hits@20: Epoch: 117, Loss: 0.1623, Valid: 63.05%, Test: 10.56%\n",
      "learning rate: 0.0014030\n",
      ": Hits@20: Epoch: 118, Loss: 0.1620, Valid: 63.59%, Test: 8.86%\n",
      "learning rate: 0.0013988\n",
      ": Hits@20: Epoch: 119, Loss: 0.1612, Valid: 65.60%, Test: 17.84%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0013946\n",
      ": Hits@20: Epoch: 120, Loss: 0.1605, Valid: 64.81%, Test: 21.80%\n",
      "learning rate: 0.0013904\n",
      ": Hits@20: Epoch: 121, Loss: 0.1603, Valid: 68.15%, Test: 11.92%\n",
      "learning rate: 0.0013862\n",
      ": Hits@20: Epoch: 122, Loss: 0.1620, Valid: 66.87%, Test: 10.89%\n",
      "learning rate: 0.0013821\n",
      ": Hits@20: Epoch: 123, Loss: 0.1594, Valid: 64.18%, Test: 23.31%\n",
      "learning rate: 0.0013779\n",
      ": Hits@20: Epoch: 124, Loss: 0.1588, Valid: 67.43%, Test: 32.12%\n",
      "learning rate: 0.0013738\n",
      ": Hits@20: Epoch: 125, Loss: 0.1596, Valid: 63.76%, Test: 11.20%\n",
      "learning rate: 0.0013697\n",
      ": Hits@20: Epoch: 126, Loss: 0.1582, Valid: 67.38%, Test: 28.81%\n",
      "learning rate: 0.0013656\n",
      ": Hits@20: Epoch: 127, Loss: 0.1591, Valid: 64.60%, Test: 32.87%\n",
      "learning rate: 0.0013615\n",
      ": Hits@20: Epoch: 128, Loss: 0.1576, Valid: 65.49%, Test: 31.77%\n",
      "learning rate: 0.0013574\n",
      ": Hits@20: Epoch: 129, Loss: 0.1570, Valid: 67.36%, Test: 13.11%\n",
      "learning rate: 0.0013533\n",
      ": Hits@20: Epoch: 130, Loss: 0.1569, Valid: 65.03%, Test: 18.99%\n",
      "learning rate: 0.0013493\n",
      ": Hits@20: Epoch: 131, Loss: 0.1569, Valid: 66.81%, Test: 30.42%\n",
      "learning rate: 0.0013452\n",
      ": Hits@20: Epoch: 132, Loss: 0.1570, Valid: 61.67%, Test: 18.41%\n",
      "learning rate: 0.0013412\n",
      ": Hits@20: Epoch: 133, Loss: 0.1578, Valid: 67.64%, Test: 14.44%\n",
      "learning rate: 0.0013372\n",
      ": Hits@20: Epoch: 134, Loss: 0.1565, Valid: 65.12%, Test: 21.88%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0013331\n",
      ": Hits@20: Epoch: 135, Loss: 0.1552, Valid: 65.42%, Test: 10.11%\n",
      "learning rate: 0.0013291\n",
      ": Hits@20: Epoch: 136, Loss: 0.1549, Valid: 67.08%, Test: 9.94%\n",
      "learning rate: 0.0013252\n",
      ": Hits@20: Epoch: 137, Loss: 0.1560, Valid: 68.06%, Test: 23.94%\n",
      "learning rate: 0.0013212\n",
      ": Hits@20: Epoch: 138, Loss: 0.1559, Valid: 66.84%, Test: 21.58%\n",
      "learning rate: 0.0013172\n",
      ": Hits@20: Epoch: 139, Loss: 0.1554, Valid: 66.31%, Test: 10.32%\n",
      "learning rate: 0.0013133\n",
      ": Hits@20: Epoch: 140, Loss: 0.1530, Valid: 67.54%, Test: 16.40%\n",
      "learning rate: 0.0013093\n",
      ": Hits@20: Epoch: 141, Loss: 0.1548, Valid: 65.80%, Test: 43.71%\n",
      "learning rate: 0.0013054\n",
      ": Hits@20: Epoch: 142, Loss: 0.1540, Valid: 62.70%, Test: 23.08%\n",
      "learning rate: 0.0013015\n",
      ": Hits@20: Epoch: 143, Loss: 0.1535, Valid: 66.08%, Test: 10.06%\n",
      "learning rate: 0.0012976\n",
      ": Hits@20: Epoch: 144, Loss: 0.1549, Valid: 63.44%, Test: 14.07%\n",
      "learning rate: 0.0012937\n",
      ": Hits@20: Epoch: 145, Loss: 0.1553, Valid: 69.14%, Test: 9.52%\n",
      "learning rate: 0.0012898\n",
      ": Hits@20: Epoch: 146, Loss: 0.1534, Valid: 66.88%, Test: 14.48%\n",
      "learning rate: 0.0012859\n",
      ": Hits@20: Epoch: 147, Loss: 0.1542, Valid: 67.79%, Test: 14.88%\n",
      "learning rate: 0.0012821\n",
      ": Hits@20: Epoch: 148, Loss: 0.1528, Valid: 67.83%, Test: 23.78%\n",
      "learning rate: 0.0012782\n",
      ": Hits@20: Epoch: 149, Loss: 0.1535, Valid: 69.37%, Test: 10.68%\n",
      "negative sampling ...\n",
      "negative sampling finished\n",
      "learning rate: 0.0012744\n",
      ": Hits@20: Epoch: 150, Loss: 0.1519, Valid: 67.81%, Test: 7.36%\n",
      "learning rate: 0.0012706\n",
      ": Hits@20: Epoch: 151, Loss: 0.1529, Valid: 70.06%, Test: 41.75%\n",
      "learning rate: 0.0012668\n",
      ": Hits@20: Epoch: 152, Loss: 0.1524, Valid: 68.95%, Test: 47.14%\n",
      "learning rate: 0.0012630\n",
      ": Hits@20: Epoch: 153, Loss: 0.1518, Valid: 68.82%, Test: 48.66%\n",
      "learning rate: 0.0012592\n",
      ": Hits@20: Epoch: 154, Loss: 0.1518, Valid: 69.11%, Test: 42.55%\n",
      "learning rate: 0.0012554\n",
      ": Hits@20: Epoch: 155, Loss: 0.1510, Valid: 69.14%, Test: 32.55%\n",
      "learning rate: 0.0012516\n",
      ": Hits@20: Epoch: 156, Loss: 0.1505, Valid: 68.86%, Test: 20.40%\n",
      "learning rate: 0.0012479\n",
      ": Hits@20: Epoch: 157, Loss: 0.1510, Valid: 68.01%, Test: 27.37%\n",
      "learning rate: 0.0012441\n",
      ": Hits@20: Epoch: 158, Loss: 0.1516, Valid: 69.92%, Test: 40.15%\n",
      "learning rate: 0.0012404\n",
      ": Hits@20: Epoch: 159, Loss: 0.1505, Valid: 70.47%, Test: 34.11%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m      2\u001b[0m     data_neg_train\u001b[38;5;241m.\u001b[39mresample_neg_edges()\n\u001b[0;32m----> 4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train(args, predictor, optimizer, scheduler, data_pos_train, data_neg_train)\n\u001b[1;32m      5\u001b[0m     lr_now \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lr_now \u001b[38;5;241m>\u001b[39m args\u001b[38;5;241m.\u001b[39mlr_mini:\n",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args, predictor, optimizer, scheduler, data_pos, data_neg)\u001b[0m\n\u001b[1;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# scheduler.step()\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     28\u001b[0m     running_examples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m running_loss \u001b[38;5;241m/\u001b[39m running_examples\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(args.epochs):\n",
    "    data_neg_train.resample_neg_edges()\n",
    "\n",
    "    loss = train(args, predictor, optimizer, scheduler, data_pos_train, data_neg_train)\n",
    "    lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "    if lr_now > args.lr_mini:\n",
    "        scheduler.step()\n",
    "\n",
    "    if epoch % args.eval_epoch == args.eval_epoch - 1:\n",
    "        # with Timing(name='Times of validation: '):\n",
    "        val_pred, val_true = test(args, predictor, data_pos_valid, data_neg_valid)\n",
    "        test_pred, test_true = test(args, predictor, data_pos_test, data_neg_test)\n",
    "        results = get_eval_result(args, val_pred, val_true, test_pred, test_true)\n",
    "        for key, result in results.items():\n",
    "            loggers[key].add_result(result)\n",
    "            valid_res, test_res = result\n",
    "            to_print = (f'learning rate: {lr_now:.7f}' + '\\n'\n",
    "                        + f': {key}: Epoch: {epoch:02d}, '\n",
    "                        + f'Loss: {loss:.4f}, Valid: {100 * valid_res:.2f}%, '\n",
    "                        + f'Test: {100 * test_res:.2f}%')\n",
    "            # tensorboard_writer.add_scalar('eval/loss', loss, epoch + 1)\n",
    "            # tensorboard_writer.add_scalar('eval/Valid', valid_res, epoch + 1)\n",
    "            # tensorboard_writer.add_scalar('eval/Test', test_res, epoch + 1)\n",
    "            with open(log_file, 'a') as f:\n",
    "                print(to_print, file=f)\n",
    "                print(to_print)\n",
    "\n",
    "\n",
    "for key in loggers.keys():\n",
    "    to_print = key\n",
    "    with open(log_file, 'a') as f:\n",
    "        print(to_print, file=f)\n",
    "        loggers[key].print_statistics(f=f)\n",
    "        print(to_print)\n",
    "        loggers[key].print_statistics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
